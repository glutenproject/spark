== Physical Plan ==
TakeOrderedAndProject (32)
+- * HashAggregate (31)
   +- Exchange (30)
      +- * HashAggregate (29)
         +- * Project (28)
            +- * BroadcastHashJoin Inner BuildRight (27)
               :- * Project (22)
               :  +- * BroadcastHashJoin Inner BuildRight (21)
               :     :- * Project (16)
               :     :  +- * BroadcastHashJoin Inner BuildRight (15)
               :     :     :- * Project (10)
               :     :     :  +- * BroadcastHashJoin Inner BuildRight (9)
               :     :     :     :- * Filter (3)
               :     :     :     :  +- * ColumnarToRow (2)
               :     :     :     :     +- Scan parquet spark_catalog.default.catalog_sales (1)
               :     :     :     +- BroadcastExchange (8)
               :     :     :        +- * Project (7)
               :     :     :           +- * Filter (6)
               :     :     :              +- * ColumnarToRow (5)
               :     :     :                 +- Scan parquet spark_catalog.default.date_dim (4)
               :     :     +- BroadcastExchange (14)
               :     :        +- * Filter (13)
               :     :           +- * ColumnarToRow (12)
               :     :              +- Scan parquet spark_catalog.default.ship_mode (11)
               :     +- BroadcastExchange (20)
               :        +- * Filter (19)
               :           +- * ColumnarToRow (18)
               :              +- Scan parquet spark_catalog.default.call_center (17)
               +- BroadcastExchange (26)
                  +- * Filter (25)
                     +- * ColumnarToRow (24)
                        +- Scan parquet spark_catalog.default.warehouse (23)


(1) Scan parquet spark_catalog.default.catalog_sales
Output [5]: [cs_ship_date_sk#1, cs_call_center_sk#2, cs_ship_mode_sk#3, cs_warehouse_sk#4, cs_sold_date_sk#5]
Batched: true
Location [not included in comparison]/{warehouse_dir}/catalog_sales]
PushedFilters: [IsNotNull(cs_warehouse_sk), IsNotNull(cs_ship_mode_sk), IsNotNull(cs_call_center_sk), IsNotNull(cs_ship_date_sk)]
ReadSchema: struct<cs_ship_date_sk:int,cs_call_center_sk:int,cs_ship_mode_sk:int,cs_warehouse_sk:int>

(2) ColumnarToRow [codegen id : 5]
Input [5]: [cs_ship_date_sk#1, cs_call_center_sk#2, cs_ship_mode_sk#3, cs_warehouse_sk#4, cs_sold_date_sk#5]

(3) Filter [codegen id : 5]
Input [5]: [cs_ship_date_sk#1, cs_call_center_sk#2, cs_ship_mode_sk#3, cs_warehouse_sk#4, cs_sold_date_sk#5]
Condition : ((((isnotnull(cs_warehouse_sk#4) AND isnotnull(cs_ship_mode_sk#3)) AND isnotnull(cs_call_center_sk#2)) AND isnotnull(cs_ship_date_sk#1)) AND might_contain(runtimefilterexpression(Subquery scalar-subquery#6, [id=#7]), xxhash64(cs_ship_date_sk#1, 42)))

(4) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_date_sk#8, d_month_seq#9]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1200), LessThanOrEqual(d_month_seq,1211), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_month_seq:int>

(5) ColumnarToRow [codegen id : 1]
Input [2]: [d_date_sk#8, d_month_seq#9]

(6) Filter [codegen id : 1]
Input [2]: [d_date_sk#8, d_month_seq#9]
Condition : (((isnotnull(d_month_seq#9) AND (d_month_seq#9 >= 1200)) AND (d_month_seq#9 <= 1211)) AND isnotnull(d_date_sk#8))

(7) Project [codegen id : 1]
Output [1]: [d_date_sk#8]
Input [2]: [d_date_sk#8, d_month_seq#9]

(8) BroadcastExchange
Input [1]: [d_date_sk#8]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=1]

(9) BroadcastHashJoin [codegen id : 5]
Left keys [1]: [cs_ship_date_sk#1]
Right keys [1]: [d_date_sk#8]
Join type: Inner
Join condition: None

(10) Project [codegen id : 5]
Output [5]: [cs_ship_date_sk#1, cs_call_center_sk#2, cs_ship_mode_sk#3, cs_warehouse_sk#4, cs_sold_date_sk#5]
Input [6]: [cs_ship_date_sk#1, cs_call_center_sk#2, cs_ship_mode_sk#3, cs_warehouse_sk#4, cs_sold_date_sk#5, d_date_sk#8]

(11) Scan parquet spark_catalog.default.ship_mode
Output [2]: [sm_ship_mode_sk#10, sm_type#11]
Batched: true
Location [not included in comparison]/{warehouse_dir}/ship_mode]
PushedFilters: [IsNotNull(sm_ship_mode_sk)]
ReadSchema: struct<sm_ship_mode_sk:int,sm_type:string>

(12) ColumnarToRow [codegen id : 2]
Input [2]: [sm_ship_mode_sk#10, sm_type#11]

(13) Filter [codegen id : 2]
Input [2]: [sm_ship_mode_sk#10, sm_type#11]
Condition : isnotnull(sm_ship_mode_sk#10)

(14) BroadcastExchange
Input [2]: [sm_ship_mode_sk#10, sm_type#11]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2]

(15) BroadcastHashJoin [codegen id : 5]
Left keys [1]: [cs_ship_mode_sk#3]
Right keys [1]: [sm_ship_mode_sk#10]
Join type: Inner
Join condition: None

(16) Project [codegen id : 5]
Output [5]: [cs_ship_date_sk#1, cs_call_center_sk#2, cs_warehouse_sk#4, cs_sold_date_sk#5, sm_type#11]
Input [7]: [cs_ship_date_sk#1, cs_call_center_sk#2, cs_ship_mode_sk#3, cs_warehouse_sk#4, cs_sold_date_sk#5, sm_ship_mode_sk#10, sm_type#11]

(17) Scan parquet spark_catalog.default.call_center
Output [2]: [cc_call_center_sk#12, cc_name#13]
Batched: true
Location [not included in comparison]/{warehouse_dir}/call_center]
PushedFilters: [IsNotNull(cc_call_center_sk)]
ReadSchema: struct<cc_call_center_sk:int,cc_name:string>

(18) ColumnarToRow [codegen id : 3]
Input [2]: [cc_call_center_sk#12, cc_name#13]

(19) Filter [codegen id : 3]
Input [2]: [cc_call_center_sk#12, cc_name#13]
Condition : isnotnull(cc_call_center_sk#12)

(20) BroadcastExchange
Input [2]: [cc_call_center_sk#12, cc_name#13]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=3]

(21) BroadcastHashJoin [codegen id : 5]
Left keys [1]: [cs_call_center_sk#2]
Right keys [1]: [cc_call_center_sk#12]
Join type: Inner
Join condition: None

(22) Project [codegen id : 5]
Output [5]: [cs_ship_date_sk#1, cs_warehouse_sk#4, cs_sold_date_sk#5, sm_type#11, cc_name#13]
Input [7]: [cs_ship_date_sk#1, cs_call_center_sk#2, cs_warehouse_sk#4, cs_sold_date_sk#5, sm_type#11, cc_call_center_sk#12, cc_name#13]

(23) Scan parquet spark_catalog.default.warehouse
Output [2]: [w_warehouse_sk#14, w_warehouse_name#15]
Batched: true
Location [not included in comparison]/{warehouse_dir}/warehouse]
PushedFilters: [IsNotNull(w_warehouse_sk)]
ReadSchema: struct<w_warehouse_sk:int,w_warehouse_name:string>

(24) ColumnarToRow [codegen id : 4]
Input [2]: [w_warehouse_sk#14, w_warehouse_name#15]

(25) Filter [codegen id : 4]
Input [2]: [w_warehouse_sk#14, w_warehouse_name#15]
Condition : isnotnull(w_warehouse_sk#14)

(26) BroadcastExchange
Input [2]: [w_warehouse_sk#14, w_warehouse_name#15]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=4]

(27) BroadcastHashJoin [codegen id : 5]
Left keys [1]: [cs_warehouse_sk#4]
Right keys [1]: [w_warehouse_sk#14]
Join type: Inner
Join condition: None

(28) Project [codegen id : 5]
Output [5]: [cs_ship_date_sk#1, cs_sold_date_sk#5, sm_type#11, cc_name#13, substr(w_warehouse_name#15, 1, 20) AS _groupingexpression#16]
Input [7]: [cs_ship_date_sk#1, cs_warehouse_sk#4, cs_sold_date_sk#5, sm_type#11, cc_name#13, w_warehouse_sk#14, w_warehouse_name#15]

(29) HashAggregate [codegen id : 5]
Input [5]: [cs_ship_date_sk#1, cs_sold_date_sk#5, sm_type#11, cc_name#13, _groupingexpression#16]
Keys [3]: [_groupingexpression#16, sm_type#11, cc_name#13]
Functions [5]: [partial_sum(CASE WHEN ((cs_ship_date_sk#1 - cs_sold_date_sk#5) <= 30) THEN 1 ELSE 0 END), partial_sum(CASE WHEN (((cs_ship_date_sk#1 - cs_sold_date_sk#5) > 30) AND ((cs_ship_date_sk#1 - cs_sold_date_sk#5) <= 60)) THEN 1 ELSE 0 END), partial_sum(CASE WHEN (((cs_ship_date_sk#1 - cs_sold_date_sk#5) > 60) AND ((cs_ship_date_sk#1 - cs_sold_date_sk#5) <= 90)) THEN 1 ELSE 0 END), partial_sum(CASE WHEN (((cs_ship_date_sk#1 - cs_sold_date_sk#5) > 90) AND ((cs_ship_date_sk#1 - cs_sold_date_sk#5) <= 120)) THEN 1 ELSE 0 END), partial_sum(CASE WHEN ((cs_ship_date_sk#1 - cs_sold_date_sk#5) > 120) THEN 1 ELSE 0 END)]
Aggregate Attributes [5]: [sum#17, sum#18, sum#19, sum#20, sum#21]
Results [8]: [_groupingexpression#16, sm_type#11, cc_name#13, sum#22, sum#23, sum#24, sum#25, sum#26]

(30) Exchange
Input [8]: [_groupingexpression#16, sm_type#11, cc_name#13, sum#22, sum#23, sum#24, sum#25, sum#26]
Arguments: hashpartitioning(_groupingexpression#16, sm_type#11, cc_name#13, 5), ENSURE_REQUIREMENTS, [plan_id=5]

(31) HashAggregate [codegen id : 6]
Input [8]: [_groupingexpression#16, sm_type#11, cc_name#13, sum#22, sum#23, sum#24, sum#25, sum#26]
Keys [3]: [_groupingexpression#16, sm_type#11, cc_name#13]
Functions [5]: [sum(CASE WHEN ((cs_ship_date_sk#1 - cs_sold_date_sk#5) <= 30) THEN 1 ELSE 0 END), sum(CASE WHEN (((cs_ship_date_sk#1 - cs_sold_date_sk#5) > 30) AND ((cs_ship_date_sk#1 - cs_sold_date_sk#5) <= 60)) THEN 1 ELSE 0 END), sum(CASE WHEN (((cs_ship_date_sk#1 - cs_sold_date_sk#5) > 60) AND ((cs_ship_date_sk#1 - cs_sold_date_sk#5) <= 90)) THEN 1 ELSE 0 END), sum(CASE WHEN (((cs_ship_date_sk#1 - cs_sold_date_sk#5) > 90) AND ((cs_ship_date_sk#1 - cs_sold_date_sk#5) <= 120)) THEN 1 ELSE 0 END), sum(CASE WHEN ((cs_ship_date_sk#1 - cs_sold_date_sk#5) > 120) THEN 1 ELSE 0 END)]
Aggregate Attributes [5]: [sum(CASE WHEN ((cs_ship_date_sk#1 - cs_sold_date_sk#5) <= 30) THEN 1 ELSE 0 END)#27, sum(CASE WHEN (((cs_ship_date_sk#1 - cs_sold_date_sk#5) > 30) AND ((cs_ship_date_sk#1 - cs_sold_date_sk#5) <= 60)) THEN 1 ELSE 0 END)#28, sum(CASE WHEN (((cs_ship_date_sk#1 - cs_sold_date_sk#5) > 60) AND ((cs_ship_date_sk#1 - cs_sold_date_sk#5) <= 90)) THEN 1 ELSE 0 END)#29, sum(CASE WHEN (((cs_ship_date_sk#1 - cs_sold_date_sk#5) > 90) AND ((cs_ship_date_sk#1 - cs_sold_date_sk#5) <= 120)) THEN 1 ELSE 0 END)#30, sum(CASE WHEN ((cs_ship_date_sk#1 - cs_sold_date_sk#5) > 120) THEN 1 ELSE 0 END)#31]
Results [8]: [_groupingexpression#16 AS substr(w_warehouse_name, 1, 20)#32, sm_type#11, cc_name#13, sum(CASE WHEN ((cs_ship_date_sk#1 - cs_sold_date_sk#5) <= 30) THEN 1 ELSE 0 END)#27 AS 30 days #33, sum(CASE WHEN (((cs_ship_date_sk#1 - cs_sold_date_sk#5) > 30) AND ((cs_ship_date_sk#1 - cs_sold_date_sk#5) <= 60)) THEN 1 ELSE 0 END)#28 AS 31 - 60 days #34, sum(CASE WHEN (((cs_ship_date_sk#1 - cs_sold_date_sk#5) > 60) AND ((cs_ship_date_sk#1 - cs_sold_date_sk#5) <= 90)) THEN 1 ELSE 0 END)#29 AS 61 - 90 days #35, sum(CASE WHEN (((cs_ship_date_sk#1 - cs_sold_date_sk#5) > 90) AND ((cs_ship_date_sk#1 - cs_sold_date_sk#5) <= 120)) THEN 1 ELSE 0 END)#30 AS 91 - 120 days #36, sum(CASE WHEN ((cs_ship_date_sk#1 - cs_sold_date_sk#5) > 120) THEN 1 ELSE 0 END)#31 AS >120 days #37]

(32) TakeOrderedAndProject
Input [8]: [substr(w_warehouse_name, 1, 20)#32, sm_type#11, cc_name#13, 30 days #33, 31 - 60 days #34, 61 - 90 days #35, 91 - 120 days #36, >120 days #37]
Arguments: 100, [substr(w_warehouse_name, 1, 20)#32 ASC NULLS FIRST, sm_type#11 ASC NULLS FIRST, cc_name#13 ASC NULLS FIRST], [substr(w_warehouse_name, 1, 20)#32, sm_type#11, cc_name#13, 30 days #33, 31 - 60 days #34, 61 - 90 days #35, 91 - 120 days #36, >120 days #37]

===== Subqueries =====

Subquery:1 Hosting operator id = 3 Hosting Expression = Subquery scalar-subquery#6, [id=#7]
ObjectHashAggregate (39)
+- Exchange (38)
   +- ObjectHashAggregate (37)
      +- * Project (36)
         +- * Filter (35)
            +- * ColumnarToRow (34)
               +- Scan parquet spark_catalog.default.date_dim (33)


(33) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_date_sk#8, d_month_seq#9]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1200), LessThanOrEqual(d_month_seq,1211), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_month_seq:int>

(34) ColumnarToRow [codegen id : 1]
Input [2]: [d_date_sk#8, d_month_seq#9]

(35) Filter [codegen id : 1]
Input [2]: [d_date_sk#8, d_month_seq#9]
Condition : (((isnotnull(d_month_seq#9) AND (d_month_seq#9 >= 1200)) AND (d_month_seq#9 <= 1211)) AND isnotnull(d_date_sk#8))

(36) Project [codegen id : 1]
Output [1]: [d_date_sk#8]
Input [2]: [d_date_sk#8, d_month_seq#9]

(37) ObjectHashAggregate
Input [1]: [d_date_sk#8]
Keys: []
Functions [1]: [partial_bloom_filter_agg(xxhash64(d_date_sk#8, 42), 335, 2680, 0, 0)]
Aggregate Attributes [1]: [buf#38]
Results [1]: [buf#39]

(38) Exchange
Input [1]: [buf#39]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=6]

(39) ObjectHashAggregate
Input [1]: [buf#39]
Keys: []
Functions [1]: [bloom_filter_agg(xxhash64(d_date_sk#8, 42), 335, 2680, 0, 0)]
Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(d_date_sk#8, 42), 335, 2680, 0, 0)#40]
Results [1]: [bloom_filter_agg(xxhash64(d_date_sk#8, 42), 335, 2680, 0, 0)#40 AS bloomFilter#41]


