== Physical Plan ==
* Sort (35)
+- Exchange (34)
   +- * Project (33)
      +- * SortMergeJoin Inner (32)
         :- * Sort (26)
         :  +- Exchange (25)
         :     +- * Filter (24)
         :        +- * HashAggregate (23)
         :           +- Exchange (22)
         :              +- * HashAggregate (21)
         :                 +- * Project (20)
         :                    +- * BroadcastHashJoin Inner BuildRight (19)
         :                       :- * Project (13)
         :                       :  +- * BroadcastHashJoin Inner BuildRight (12)
         :                       :     :- * Project (6)
         :                       :     :  +- * BroadcastHashJoin Inner BuildRight (5)
         :                       :     :     :- * Filter (3)
         :                       :     :     :  +- * ColumnarToRow (2)
         :                       :     :     :     +- Scan parquet spark_catalog.default.store_sales (1)
         :                       :     :     +- ReusedExchange (4)
         :                       :     +- BroadcastExchange (11)
         :                       :        +- * Project (10)
         :                       :           +- * Filter (9)
         :                       :              +- * ColumnarToRow (8)
         :                       :                 +- Scan parquet spark_catalog.default.store (7)
         :                       +- BroadcastExchange (18)
         :                          +- * Project (17)
         :                             +- * Filter (16)
         :                                +- * ColumnarToRow (15)
         :                                   +- Scan parquet spark_catalog.default.household_demographics (14)
         +- * Sort (31)
            +- Exchange (30)
               +- * Filter (29)
                  +- * ColumnarToRow (28)
                     +- Scan parquet spark_catalog.default.customer (27)


(1) Scan parquet spark_catalog.default.store_sales
Output [5]: [ss_customer_sk#1, ss_hdemo_sk#2, ss_store_sk#3, ss_ticket_number#4, ss_sold_date_sk#5]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ss_sold_date_sk#5), dynamicpruningexpression(ss_sold_date_sk#5 IN dynamicpruning#6)]
PushedFilters: [IsNotNull(ss_store_sk), IsNotNull(ss_hdemo_sk), IsNotNull(ss_customer_sk)]
ReadSchema: struct<ss_customer_sk:int,ss_hdemo_sk:int,ss_store_sk:int,ss_ticket_number:int>

(2) ColumnarToRow [codegen id : 4]
Input [5]: [ss_customer_sk#1, ss_hdemo_sk#2, ss_store_sk#3, ss_ticket_number#4, ss_sold_date_sk#5]

(3) Filter [codegen id : 4]
Input [5]: [ss_customer_sk#1, ss_hdemo_sk#2, ss_store_sk#3, ss_ticket_number#4, ss_sold_date_sk#5]
Condition : ((((isnotnull(ss_store_sk#3) AND isnotnull(ss_hdemo_sk#2)) AND isnotnull(ss_customer_sk#1)) AND might_contain(runtimefilterexpression(Subquery scalar-subquery#7, [id=#8]), xxhash64(ss_store_sk#3, 42))) AND might_contain(runtimefilterexpression(Subquery scalar-subquery#9, [id=#10]), xxhash64(ss_hdemo_sk#2, 42)))

(4) ReusedExchange [Reuses operator id: 54]
Output [1]: [d_date_sk#11]

(5) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [ss_sold_date_sk#5]
Right keys [1]: [d_date_sk#11]
Join type: Inner
Join condition: None

(6) Project [codegen id : 4]
Output [4]: [ss_customer_sk#1, ss_hdemo_sk#2, ss_store_sk#3, ss_ticket_number#4]
Input [6]: [ss_customer_sk#1, ss_hdemo_sk#2, ss_store_sk#3, ss_ticket_number#4, ss_sold_date_sk#5, d_date_sk#11]

(7) Scan parquet spark_catalog.default.store
Output [2]: [s_store_sk#12, s_county#13]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store]
PushedFilters: [IsNotNull(s_county), EqualTo(s_county,Williamson County), IsNotNull(s_store_sk)]
ReadSchema: struct<s_store_sk:int,s_county:string>

(8) ColumnarToRow [codegen id : 2]
Input [2]: [s_store_sk#12, s_county#13]

(9) Filter [codegen id : 2]
Input [2]: [s_store_sk#12, s_county#13]
Condition : ((isnotnull(s_county#13) AND (s_county#13 = Williamson County)) AND isnotnull(s_store_sk#12))

(10) Project [codegen id : 2]
Output [1]: [s_store_sk#12]
Input [2]: [s_store_sk#12, s_county#13]

(11) BroadcastExchange
Input [1]: [s_store_sk#12]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=1]

(12) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [ss_store_sk#3]
Right keys [1]: [s_store_sk#12]
Join type: Inner
Join condition: None

(13) Project [codegen id : 4]
Output [3]: [ss_customer_sk#1, ss_hdemo_sk#2, ss_ticket_number#4]
Input [5]: [ss_customer_sk#1, ss_hdemo_sk#2, ss_store_sk#3, ss_ticket_number#4, s_store_sk#12]

(14) Scan parquet spark_catalog.default.household_demographics
Output [4]: [hd_demo_sk#14, hd_buy_potential#15, hd_dep_count#16, hd_vehicle_count#17]
Batched: true
Location [not included in comparison]/{warehouse_dir}/household_demographics]
PushedFilters: [IsNotNull(hd_vehicle_count), Or(EqualTo(hd_buy_potential,>10000         ),EqualTo(hd_buy_potential,unknown        )), GreaterThan(hd_vehicle_count,0), IsNotNull(hd_demo_sk)]
ReadSchema: struct<hd_demo_sk:int,hd_buy_potential:string,hd_dep_count:int,hd_vehicle_count:int>

(15) ColumnarToRow [codegen id : 3]
Input [4]: [hd_demo_sk#14, hd_buy_potential#15, hd_dep_count#16, hd_vehicle_count#17]

(16) Filter [codegen id : 3]
Input [4]: [hd_demo_sk#14, hd_buy_potential#15, hd_dep_count#16, hd_vehicle_count#17]
Condition : ((((isnotnull(hd_vehicle_count#17) AND ((hd_buy_potential#15 = >10000         ) OR (hd_buy_potential#15 = unknown        ))) AND (hd_vehicle_count#17 > 0)) AND CASE WHEN (hd_vehicle_count#17 > 0) THEN ((cast(hd_dep_count#16 as double) / cast(hd_vehicle_count#17 as double)) > 1.2) END) AND isnotnull(hd_demo_sk#14))

(17) Project [codegen id : 3]
Output [1]: [hd_demo_sk#14]
Input [4]: [hd_demo_sk#14, hd_buy_potential#15, hd_dep_count#16, hd_vehicle_count#17]

(18) BroadcastExchange
Input [1]: [hd_demo_sk#14]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=2]

(19) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [ss_hdemo_sk#2]
Right keys [1]: [hd_demo_sk#14]
Join type: Inner
Join condition: None

(20) Project [codegen id : 4]
Output [2]: [ss_customer_sk#1, ss_ticket_number#4]
Input [4]: [ss_customer_sk#1, ss_hdemo_sk#2, ss_ticket_number#4, hd_demo_sk#14]

(21) HashAggregate [codegen id : 4]
Input [2]: [ss_customer_sk#1, ss_ticket_number#4]
Keys [2]: [ss_ticket_number#4, ss_customer_sk#1]
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#18]
Results [3]: [ss_ticket_number#4, ss_customer_sk#1, count#19]

(22) Exchange
Input [3]: [ss_ticket_number#4, ss_customer_sk#1, count#19]
Arguments: hashpartitioning(ss_ticket_number#4, ss_customer_sk#1, 5), ENSURE_REQUIREMENTS, [plan_id=3]

(23) HashAggregate [codegen id : 5]
Input [3]: [ss_ticket_number#4, ss_customer_sk#1, count#19]
Keys [2]: [ss_ticket_number#4, ss_customer_sk#1]
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#20]
Results [3]: [ss_ticket_number#4, ss_customer_sk#1, count(1)#20 AS cnt#21]

(24) Filter [codegen id : 5]
Input [3]: [ss_ticket_number#4, ss_customer_sk#1, cnt#21]
Condition : ((cnt#21 >= 15) AND (cnt#21 <= 20))

(25) Exchange
Input [3]: [ss_ticket_number#4, ss_customer_sk#1, cnt#21]
Arguments: hashpartitioning(ss_customer_sk#1, 5), ENSURE_REQUIREMENTS, [plan_id=4]

(26) Sort [codegen id : 6]
Input [3]: [ss_ticket_number#4, ss_customer_sk#1, cnt#21]
Arguments: [ss_customer_sk#1 ASC NULLS FIRST], false, 0

(27) Scan parquet spark_catalog.default.customer
Output [5]: [c_customer_sk#22, c_salutation#23, c_first_name#24, c_last_name#25, c_preferred_cust_flag#26]
Batched: true
Location [not included in comparison]/{warehouse_dir}/customer]
PushedFilters: [IsNotNull(c_customer_sk)]
ReadSchema: struct<c_customer_sk:int,c_salutation:string,c_first_name:string,c_last_name:string,c_preferred_cust_flag:string>

(28) ColumnarToRow [codegen id : 7]
Input [5]: [c_customer_sk#22, c_salutation#23, c_first_name#24, c_last_name#25, c_preferred_cust_flag#26]

(29) Filter [codegen id : 7]
Input [5]: [c_customer_sk#22, c_salutation#23, c_first_name#24, c_last_name#25, c_preferred_cust_flag#26]
Condition : isnotnull(c_customer_sk#22)

(30) Exchange
Input [5]: [c_customer_sk#22, c_salutation#23, c_first_name#24, c_last_name#25, c_preferred_cust_flag#26]
Arguments: hashpartitioning(c_customer_sk#22, 5), ENSURE_REQUIREMENTS, [plan_id=5]

(31) Sort [codegen id : 8]
Input [5]: [c_customer_sk#22, c_salutation#23, c_first_name#24, c_last_name#25, c_preferred_cust_flag#26]
Arguments: [c_customer_sk#22 ASC NULLS FIRST], false, 0

(32) SortMergeJoin [codegen id : 9]
Left keys [1]: [ss_customer_sk#1]
Right keys [1]: [c_customer_sk#22]
Join type: Inner
Join condition: None

(33) Project [codegen id : 9]
Output [6]: [c_last_name#25, c_first_name#24, c_salutation#23, c_preferred_cust_flag#26, ss_ticket_number#4, cnt#21]
Input [8]: [ss_ticket_number#4, ss_customer_sk#1, cnt#21, c_customer_sk#22, c_salutation#23, c_first_name#24, c_last_name#25, c_preferred_cust_flag#26]

(34) Exchange
Input [6]: [c_last_name#25, c_first_name#24, c_salutation#23, c_preferred_cust_flag#26, ss_ticket_number#4, cnt#21]
Arguments: rangepartitioning(c_last_name#25 ASC NULLS FIRST, c_first_name#24 ASC NULLS FIRST, c_salutation#23 ASC NULLS FIRST, c_preferred_cust_flag#26 DESC NULLS LAST, 5), ENSURE_REQUIREMENTS, [plan_id=6]

(35) Sort [codegen id : 10]
Input [6]: [c_last_name#25, c_first_name#24, c_salutation#23, c_preferred_cust_flag#26, ss_ticket_number#4, cnt#21]
Arguments: [c_last_name#25 ASC NULLS FIRST, c_first_name#24 ASC NULLS FIRST, c_salutation#23 ASC NULLS FIRST, c_preferred_cust_flag#26 DESC NULLS LAST], true, 0

===== Subqueries =====

Subquery:1 Hosting operator id = 3 Hosting Expression = Subquery scalar-subquery#7, [id=#8]
ObjectHashAggregate (42)
+- Exchange (41)
   +- ObjectHashAggregate (40)
      +- * Project (39)
         +- * Filter (38)
            +- * ColumnarToRow (37)
               +- Scan parquet spark_catalog.default.store (36)


(36) Scan parquet spark_catalog.default.store
Output [2]: [s_store_sk#12, s_county#13]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store]
PushedFilters: [IsNotNull(s_county), EqualTo(s_county,Williamson County), IsNotNull(s_store_sk)]
ReadSchema: struct<s_store_sk:int,s_county:string>

(37) ColumnarToRow [codegen id : 1]
Input [2]: [s_store_sk#12, s_county#13]

(38) Filter [codegen id : 1]
Input [2]: [s_store_sk#12, s_county#13]
Condition : ((isnotnull(s_county#13) AND (s_county#13 = Williamson County)) AND isnotnull(s_store_sk#12))

(39) Project [codegen id : 1]
Output [1]: [s_store_sk#12]
Input [2]: [s_store_sk#12, s_county#13]

(40) ObjectHashAggregate
Input [1]: [s_store_sk#12]
Keys: []
Functions [1]: [partial_bloom_filter_agg(xxhash64(s_store_sk#12, 42), 45, 360, 0, 0)]
Aggregate Attributes [1]: [buf#27]
Results [1]: [buf#28]

(41) Exchange
Input [1]: [buf#28]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=7]

(42) ObjectHashAggregate
Input [1]: [buf#28]
Keys: []
Functions [1]: [bloom_filter_agg(xxhash64(s_store_sk#12, 42), 45, 360, 0, 0)]
Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(s_store_sk#12, 42), 45, 360, 0, 0)#29]
Results [1]: [bloom_filter_agg(xxhash64(s_store_sk#12, 42), 45, 360, 0, 0)#29 AS bloomFilter#30]

Subquery:2 Hosting operator id = 3 Hosting Expression = Subquery scalar-subquery#9, [id=#10]
ObjectHashAggregate (49)
+- Exchange (48)
   +- ObjectHashAggregate (47)
      +- * Project (46)
         +- * Filter (45)
            +- * ColumnarToRow (44)
               +- Scan parquet spark_catalog.default.household_demographics (43)


(43) Scan parquet spark_catalog.default.household_demographics
Output [4]: [hd_demo_sk#14, hd_buy_potential#15, hd_dep_count#16, hd_vehicle_count#17]
Batched: true
Location [not included in comparison]/{warehouse_dir}/household_demographics]
PushedFilters: [IsNotNull(hd_vehicle_count), Or(EqualTo(hd_buy_potential,>10000         ),EqualTo(hd_buy_potential,unknown        )), GreaterThan(hd_vehicle_count,0), IsNotNull(hd_demo_sk)]
ReadSchema: struct<hd_demo_sk:int,hd_buy_potential:string,hd_dep_count:int,hd_vehicle_count:int>

(44) ColumnarToRow [codegen id : 1]
Input [4]: [hd_demo_sk#14, hd_buy_potential#15, hd_dep_count#16, hd_vehicle_count#17]

(45) Filter [codegen id : 1]
Input [4]: [hd_demo_sk#14, hd_buy_potential#15, hd_dep_count#16, hd_vehicle_count#17]
Condition : ((((isnotnull(hd_vehicle_count#17) AND ((hd_buy_potential#15 = >10000         ) OR (hd_buy_potential#15 = unknown        ))) AND (hd_vehicle_count#17 > 0)) AND CASE WHEN (hd_vehicle_count#17 > 0) THEN ((cast(hd_dep_count#16 as double) / cast(hd_vehicle_count#17 as double)) > 1.2) END) AND isnotnull(hd_demo_sk#14))

(46) Project [codegen id : 1]
Output [1]: [hd_demo_sk#14]
Input [4]: [hd_demo_sk#14, hd_buy_potential#15, hd_dep_count#16, hd_vehicle_count#17]

(47) ObjectHashAggregate
Input [1]: [hd_demo_sk#14]
Keys: []
Functions [1]: [partial_bloom_filter_agg(xxhash64(hd_demo_sk#14, 42), 1760, 14080, 0, 0)]
Aggregate Attributes [1]: [buf#31]
Results [1]: [buf#32]

(48) Exchange
Input [1]: [buf#32]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=8]

(49) ObjectHashAggregate
Input [1]: [buf#32]
Keys: []
Functions [1]: [bloom_filter_agg(xxhash64(hd_demo_sk#14, 42), 1760, 14080, 0, 0)]
Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(hd_demo_sk#14, 42), 1760, 14080, 0, 0)#33]
Results [1]: [bloom_filter_agg(xxhash64(hd_demo_sk#14, 42), 1760, 14080, 0, 0)#33 AS bloomFilter#34]

Subquery:3 Hosting operator id = 1 Hosting Expression = ss_sold_date_sk#5 IN dynamicpruning#6
BroadcastExchange (54)
+- * Project (53)
   +- * Filter (52)
      +- * ColumnarToRow (51)
         +- Scan parquet spark_catalog.default.date_dim (50)


(50) Scan parquet spark_catalog.default.date_dim
Output [3]: [d_date_sk#11, d_year#35, d_dom#36]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [Or(And(GreaterThanOrEqual(d_dom,1),LessThanOrEqual(d_dom,3)),And(GreaterThanOrEqual(d_dom,25),LessThanOrEqual(d_dom,28))), In(d_year, [1999,2000,2001]), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_year:int,d_dom:int>

(51) ColumnarToRow [codegen id : 1]
Input [3]: [d_date_sk#11, d_year#35, d_dom#36]

(52) Filter [codegen id : 1]
Input [3]: [d_date_sk#11, d_year#35, d_dom#36]
Condition : (((((d_dom#36 >= 1) AND (d_dom#36 <= 3)) OR ((d_dom#36 >= 25) AND (d_dom#36 <= 28))) AND d_year#35 IN (1999,2000,2001)) AND isnotnull(d_date_sk#11))

(53) Project [codegen id : 1]
Output [1]: [d_date_sk#11]
Input [3]: [d_date_sk#11, d_year#35, d_dom#36]

(54) BroadcastExchange
Input [1]: [d_date_sk#11]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=9]


