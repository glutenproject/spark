== Physical Plan ==
* BroadcastNestedLoopJoin Inner BuildRight (182)
:- * BroadcastNestedLoopJoin Inner BuildRight (160)
:  :- * BroadcastNestedLoopJoin Inner BuildRight (138)
:  :  :- * BroadcastNestedLoopJoin Inner BuildRight (116)
:  :  :  :- * BroadcastNestedLoopJoin Inner BuildRight (94)
:  :  :  :  :- * BroadcastNestedLoopJoin Inner BuildRight (72)
:  :  :  :  :  :- * BroadcastNestedLoopJoin Inner BuildRight (50)
:  :  :  :  :  :  :- * HashAggregate (28)
:  :  :  :  :  :  :  +- Exchange (27)
:  :  :  :  :  :  :     +- * HashAggregate (26)
:  :  :  :  :  :  :        +- * Project (25)
:  :  :  :  :  :  :           +- * BroadcastHashJoin Inner BuildRight (24)
:  :  :  :  :  :  :              :- * Project (18)
:  :  :  :  :  :  :              :  +- * BroadcastHashJoin Inner BuildRight (17)
:  :  :  :  :  :  :              :     :- * Project (11)
:  :  :  :  :  :  :              :     :  +- * BroadcastHashJoin Inner BuildRight (10)
:  :  :  :  :  :  :              :     :     :- * Project (4)
:  :  :  :  :  :  :              :     :     :  +- * Filter (3)
:  :  :  :  :  :  :              :     :     :     +- * ColumnarToRow (2)
:  :  :  :  :  :  :              :     :     :        +- Scan parquet spark_catalog.default.store_sales (1)
:  :  :  :  :  :  :              :     :     +- BroadcastExchange (9)
:  :  :  :  :  :  :              :     :        +- * Project (8)
:  :  :  :  :  :  :              :     :           +- * Filter (7)
:  :  :  :  :  :  :              :     :              +- * ColumnarToRow (6)
:  :  :  :  :  :  :              :     :                 +- Scan parquet spark_catalog.default.time_dim (5)
:  :  :  :  :  :  :              :     +- BroadcastExchange (16)
:  :  :  :  :  :  :              :        +- * Project (15)
:  :  :  :  :  :  :              :           +- * Filter (14)
:  :  :  :  :  :  :              :              +- * ColumnarToRow (13)
:  :  :  :  :  :  :              :                 +- Scan parquet spark_catalog.default.store (12)
:  :  :  :  :  :  :              +- BroadcastExchange (23)
:  :  :  :  :  :  :                 +- * Project (22)
:  :  :  :  :  :  :                    +- * Filter (21)
:  :  :  :  :  :  :                       +- * ColumnarToRow (20)
:  :  :  :  :  :  :                          +- Scan parquet spark_catalog.default.household_demographics (19)
:  :  :  :  :  :  +- BroadcastExchange (49)
:  :  :  :  :  :     +- * HashAggregate (48)
:  :  :  :  :  :        +- Exchange (47)
:  :  :  :  :  :           +- * HashAggregate (46)
:  :  :  :  :  :              +- * Project (45)
:  :  :  :  :  :                 +- * BroadcastHashJoin Inner BuildRight (44)
:  :  :  :  :  :                    :- * Project (42)
:  :  :  :  :  :                    :  +- * BroadcastHashJoin Inner BuildRight (41)
:  :  :  :  :  :                    :     :- * Project (39)
:  :  :  :  :  :                    :     :  +- * BroadcastHashJoin Inner BuildRight (38)
:  :  :  :  :  :                    :     :     :- * Project (32)
:  :  :  :  :  :                    :     :     :  +- * Filter (31)
:  :  :  :  :  :                    :     :     :     +- * ColumnarToRow (30)
:  :  :  :  :  :                    :     :     :        +- Scan parquet spark_catalog.default.store_sales (29)
:  :  :  :  :  :                    :     :     +- BroadcastExchange (37)
:  :  :  :  :  :                    :     :        +- * Project (36)
:  :  :  :  :  :                    :     :           +- * Filter (35)
:  :  :  :  :  :                    :     :              +- * ColumnarToRow (34)
:  :  :  :  :  :                    :     :                 +- Scan parquet spark_catalog.default.time_dim (33)
:  :  :  :  :  :                    :     +- ReusedExchange (40)
:  :  :  :  :  :                    +- ReusedExchange (43)
:  :  :  :  :  +- BroadcastExchange (71)
:  :  :  :  :     +- * HashAggregate (70)
:  :  :  :  :        +- Exchange (69)
:  :  :  :  :           +- * HashAggregate (68)
:  :  :  :  :              +- * Project (67)
:  :  :  :  :                 +- * BroadcastHashJoin Inner BuildRight (66)
:  :  :  :  :                    :- * Project (64)
:  :  :  :  :                    :  +- * BroadcastHashJoin Inner BuildRight (63)
:  :  :  :  :                    :     :- * Project (61)
:  :  :  :  :                    :     :  +- * BroadcastHashJoin Inner BuildRight (60)
:  :  :  :  :                    :     :     :- * Project (54)
:  :  :  :  :                    :     :     :  +- * Filter (53)
:  :  :  :  :                    :     :     :     +- * ColumnarToRow (52)
:  :  :  :  :                    :     :     :        +- Scan parquet spark_catalog.default.store_sales (51)
:  :  :  :  :                    :     :     +- BroadcastExchange (59)
:  :  :  :  :                    :     :        +- * Project (58)
:  :  :  :  :                    :     :           +- * Filter (57)
:  :  :  :  :                    :     :              +- * ColumnarToRow (56)
:  :  :  :  :                    :     :                 +- Scan parquet spark_catalog.default.time_dim (55)
:  :  :  :  :                    :     +- ReusedExchange (62)
:  :  :  :  :                    +- ReusedExchange (65)
:  :  :  :  +- BroadcastExchange (93)
:  :  :  :     +- * HashAggregate (92)
:  :  :  :        +- Exchange (91)
:  :  :  :           +- * HashAggregate (90)
:  :  :  :              +- * Project (89)
:  :  :  :                 +- * BroadcastHashJoin Inner BuildRight (88)
:  :  :  :                    :- * Project (86)
:  :  :  :                    :  +- * BroadcastHashJoin Inner BuildRight (85)
:  :  :  :                    :     :- * Project (83)
:  :  :  :                    :     :  +- * BroadcastHashJoin Inner BuildRight (82)
:  :  :  :                    :     :     :- * Project (76)
:  :  :  :                    :     :     :  +- * Filter (75)
:  :  :  :                    :     :     :     +- * ColumnarToRow (74)
:  :  :  :                    :     :     :        +- Scan parquet spark_catalog.default.store_sales (73)
:  :  :  :                    :     :     +- BroadcastExchange (81)
:  :  :  :                    :     :        +- * Project (80)
:  :  :  :                    :     :           +- * Filter (79)
:  :  :  :                    :     :              +- * ColumnarToRow (78)
:  :  :  :                    :     :                 +- Scan parquet spark_catalog.default.time_dim (77)
:  :  :  :                    :     +- ReusedExchange (84)
:  :  :  :                    +- ReusedExchange (87)
:  :  :  +- BroadcastExchange (115)
:  :  :     +- * HashAggregate (114)
:  :  :        +- Exchange (113)
:  :  :           +- * HashAggregate (112)
:  :  :              +- * Project (111)
:  :  :                 +- * BroadcastHashJoin Inner BuildRight (110)
:  :  :                    :- * Project (108)
:  :  :                    :  +- * BroadcastHashJoin Inner BuildRight (107)
:  :  :                    :     :- * Project (105)
:  :  :                    :     :  +- * BroadcastHashJoin Inner BuildRight (104)
:  :  :                    :     :     :- * Project (98)
:  :  :                    :     :     :  +- * Filter (97)
:  :  :                    :     :     :     +- * ColumnarToRow (96)
:  :  :                    :     :     :        +- Scan parquet spark_catalog.default.store_sales (95)
:  :  :                    :     :     +- BroadcastExchange (103)
:  :  :                    :     :        +- * Project (102)
:  :  :                    :     :           +- * Filter (101)
:  :  :                    :     :              +- * ColumnarToRow (100)
:  :  :                    :     :                 +- Scan parquet spark_catalog.default.time_dim (99)
:  :  :                    :     +- ReusedExchange (106)
:  :  :                    +- ReusedExchange (109)
:  :  +- BroadcastExchange (137)
:  :     +- * HashAggregate (136)
:  :        +- Exchange (135)
:  :           +- * HashAggregate (134)
:  :              +- * Project (133)
:  :                 +- * BroadcastHashJoin Inner BuildRight (132)
:  :                    :- * Project (130)
:  :                    :  +- * BroadcastHashJoin Inner BuildRight (129)
:  :                    :     :- * Project (127)
:  :                    :     :  +- * BroadcastHashJoin Inner BuildRight (126)
:  :                    :     :     :- * Project (120)
:  :                    :     :     :  +- * Filter (119)
:  :                    :     :     :     +- * ColumnarToRow (118)
:  :                    :     :     :        +- Scan parquet spark_catalog.default.store_sales (117)
:  :                    :     :     +- BroadcastExchange (125)
:  :                    :     :        +- * Project (124)
:  :                    :     :           +- * Filter (123)
:  :                    :     :              +- * ColumnarToRow (122)
:  :                    :     :                 +- Scan parquet spark_catalog.default.time_dim (121)
:  :                    :     +- ReusedExchange (128)
:  :                    +- ReusedExchange (131)
:  +- BroadcastExchange (159)
:     +- * HashAggregate (158)
:        +- Exchange (157)
:           +- * HashAggregate (156)
:              +- * Project (155)
:                 +- * BroadcastHashJoin Inner BuildRight (154)
:                    :- * Project (152)
:                    :  +- * BroadcastHashJoin Inner BuildRight (151)
:                    :     :- * Project (149)
:                    :     :  +- * BroadcastHashJoin Inner BuildRight (148)
:                    :     :     :- * Project (142)
:                    :     :     :  +- * Filter (141)
:                    :     :     :     +- * ColumnarToRow (140)
:                    :     :     :        +- Scan parquet spark_catalog.default.store_sales (139)
:                    :     :     +- BroadcastExchange (147)
:                    :     :        +- * Project (146)
:                    :     :           +- * Filter (145)
:                    :     :              +- * ColumnarToRow (144)
:                    :     :                 +- Scan parquet spark_catalog.default.time_dim (143)
:                    :     +- ReusedExchange (150)
:                    +- ReusedExchange (153)
+- BroadcastExchange (181)
   +- * HashAggregate (180)
      +- Exchange (179)
         +- * HashAggregate (178)
            +- * Project (177)
               +- * BroadcastHashJoin Inner BuildRight (176)
                  :- * Project (174)
                  :  +- * BroadcastHashJoin Inner BuildRight (173)
                  :     :- * Project (171)
                  :     :  +- * BroadcastHashJoin Inner BuildRight (170)
                  :     :     :- * Project (164)
                  :     :     :  +- * Filter (163)
                  :     :     :     +- * ColumnarToRow (162)
                  :     :     :        +- Scan parquet spark_catalog.default.store_sales (161)
                  :     :     +- BroadcastExchange (169)
                  :     :        +- * Project (168)
                  :     :           +- * Filter (167)
                  :     :              +- * ColumnarToRow (166)
                  :     :                 +- Scan parquet spark_catalog.default.time_dim (165)
                  :     +- ReusedExchange (172)
                  +- ReusedExchange (175)


(1) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_sold_time_sk#1, ss_hdemo_sk#2, ss_store_sk#3, ss_sold_date_sk#4]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>

(2) ColumnarToRow [codegen id : 4]
Input [4]: [ss_sold_time_sk#1, ss_hdemo_sk#2, ss_store_sk#3, ss_sold_date_sk#4]

(3) Filter [codegen id : 4]
Input [4]: [ss_sold_time_sk#1, ss_hdemo_sk#2, ss_store_sk#3, ss_sold_date_sk#4]
Condition : (((((isnotnull(ss_hdemo_sk#2) AND isnotnull(ss_sold_time_sk#1)) AND isnotnull(ss_store_sk#3)) AND might_contain(runtimefilterexpression(Subquery scalar-subquery#5, [id=#6]), xxhash64(ss_sold_time_sk#1, 42))) AND might_contain(runtimefilterexpression(Subquery scalar-subquery#7, [id=#8]), xxhash64(ss_store_sk#3, 42))) AND might_contain(runtimefilterexpression(Subquery scalar-subquery#9, [id=#10]), xxhash64(ss_hdemo_sk#2, 42)))

(4) Project [codegen id : 4]
Output [3]: [ss_sold_time_sk#1, ss_hdemo_sk#2, ss_store_sk#3]
Input [4]: [ss_sold_time_sk#1, ss_hdemo_sk#2, ss_store_sk#3, ss_sold_date_sk#4]

(5) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#11, t_hour#12, t_minute#13]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,8), GreaterThanOrEqual(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(6) ColumnarToRow [codegen id : 1]
Input [3]: [t_time_sk#11, t_hour#12, t_minute#13]

(7) Filter [codegen id : 1]
Input [3]: [t_time_sk#11, t_hour#12, t_minute#13]
Condition : ((((isnotnull(t_hour#12) AND isnotnull(t_minute#13)) AND (t_hour#12 = 8)) AND (t_minute#13 >= 30)) AND isnotnull(t_time_sk#11))

(8) Project [codegen id : 1]
Output [1]: [t_time_sk#11]
Input [3]: [t_time_sk#11, t_hour#12, t_minute#13]

(9) BroadcastExchange
Input [1]: [t_time_sk#11]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=1]

(10) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [ss_sold_time_sk#1]
Right keys [1]: [t_time_sk#11]
Join type: Inner
Join condition: None

(11) Project [codegen id : 4]
Output [2]: [ss_hdemo_sk#2, ss_store_sk#3]
Input [4]: [ss_sold_time_sk#1, ss_hdemo_sk#2, ss_store_sk#3, t_time_sk#11]

(12) Scan parquet spark_catalog.default.store
Output [2]: [s_store_sk#14, s_store_name#15]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store]
PushedFilters: [IsNotNull(s_store_name), EqualTo(s_store_name,ese), IsNotNull(s_store_sk)]
ReadSchema: struct<s_store_sk:int,s_store_name:string>

(13) ColumnarToRow [codegen id : 2]
Input [2]: [s_store_sk#14, s_store_name#15]

(14) Filter [codegen id : 2]
Input [2]: [s_store_sk#14, s_store_name#15]
Condition : ((isnotnull(s_store_name#15) AND (s_store_name#15 = ese)) AND isnotnull(s_store_sk#14))

(15) Project [codegen id : 2]
Output [1]: [s_store_sk#14]
Input [2]: [s_store_sk#14, s_store_name#15]

(16) BroadcastExchange
Input [1]: [s_store_sk#14]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=2]

(17) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [ss_store_sk#3]
Right keys [1]: [s_store_sk#14]
Join type: Inner
Join condition: None

(18) Project [codegen id : 4]
Output [1]: [ss_hdemo_sk#2]
Input [3]: [ss_hdemo_sk#2, ss_store_sk#3, s_store_sk#14]

(19) Scan parquet spark_catalog.default.household_demographics
Output [3]: [hd_demo_sk#16, hd_dep_count#17, hd_vehicle_count#18]
Batched: true
Location [not included in comparison]/{warehouse_dir}/household_demographics]
PushedFilters: [Or(Or(And(EqualTo(hd_dep_count,4),LessThanOrEqual(hd_vehicle_count,6)),And(EqualTo(hd_dep_count,2),LessThanOrEqual(hd_vehicle_count,4))),And(EqualTo(hd_dep_count,0),LessThanOrEqual(hd_vehicle_count,2))), IsNotNull(hd_demo_sk)]
ReadSchema: struct<hd_demo_sk:int,hd_dep_count:int,hd_vehicle_count:int>

(20) ColumnarToRow [codegen id : 3]
Input [3]: [hd_demo_sk#16, hd_dep_count#17, hd_vehicle_count#18]

(21) Filter [codegen id : 3]
Input [3]: [hd_demo_sk#16, hd_dep_count#17, hd_vehicle_count#18]
Condition : (((((hd_dep_count#17 = 4) AND (hd_vehicle_count#18 <= 6)) OR ((hd_dep_count#17 = 2) AND (hd_vehicle_count#18 <= 4))) OR ((hd_dep_count#17 = 0) AND (hd_vehicle_count#18 <= 2))) AND isnotnull(hd_demo_sk#16))

(22) Project [codegen id : 3]
Output [1]: [hd_demo_sk#16]
Input [3]: [hd_demo_sk#16, hd_dep_count#17, hd_vehicle_count#18]

(23) BroadcastExchange
Input [1]: [hd_demo_sk#16]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=3]

(24) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [ss_hdemo_sk#2]
Right keys [1]: [hd_demo_sk#16]
Join type: Inner
Join condition: None

(25) Project [codegen id : 4]
Output: []
Input [2]: [ss_hdemo_sk#2, hd_demo_sk#16]

(26) HashAggregate [codegen id : 4]
Input: []
Keys: []
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#19]
Results [1]: [count#20]

(27) Exchange
Input [1]: [count#20]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=4]

(28) HashAggregate [codegen id : 40]
Input [1]: [count#20]
Keys: []
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#21]
Results [1]: [count(1)#21 AS h8_30_to_9#22]

(29) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_sold_time_sk#23, ss_hdemo_sk#24, ss_store_sk#25, ss_sold_date_sk#26]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>

(30) ColumnarToRow [codegen id : 8]
Input [4]: [ss_sold_time_sk#23, ss_hdemo_sk#24, ss_store_sk#25, ss_sold_date_sk#26]

(31) Filter [codegen id : 8]
Input [4]: [ss_sold_time_sk#23, ss_hdemo_sk#24, ss_store_sk#25, ss_sold_date_sk#26]
Condition : (((((isnotnull(ss_hdemo_sk#24) AND isnotnull(ss_sold_time_sk#23)) AND isnotnull(ss_store_sk#25)) AND might_contain(runtimefilterexpression(Subquery scalar-subquery#27, [id=#28]), xxhash64(ss_sold_time_sk#23, 42))) AND might_contain(runtimefilterexpression(ReusedSubquery Subquery scalar-subquery#7, [id=#8]), xxhash64(ss_store_sk#25, 42))) AND might_contain(runtimefilterexpression(ReusedSubquery Subquery scalar-subquery#9, [id=#10]), xxhash64(ss_hdemo_sk#24, 42)))

(32) Project [codegen id : 8]
Output [3]: [ss_sold_time_sk#23, ss_hdemo_sk#24, ss_store_sk#25]
Input [4]: [ss_sold_time_sk#23, ss_hdemo_sk#24, ss_store_sk#25, ss_sold_date_sk#26]

(33) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#29, t_hour#30, t_minute#31]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,9), LessThan(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(34) ColumnarToRow [codegen id : 5]
Input [3]: [t_time_sk#29, t_hour#30, t_minute#31]

(35) Filter [codegen id : 5]
Input [3]: [t_time_sk#29, t_hour#30, t_minute#31]
Condition : ((((isnotnull(t_hour#30) AND isnotnull(t_minute#31)) AND (t_hour#30 = 9)) AND (t_minute#31 < 30)) AND isnotnull(t_time_sk#29))

(36) Project [codegen id : 5]
Output [1]: [t_time_sk#29]
Input [3]: [t_time_sk#29, t_hour#30, t_minute#31]

(37) BroadcastExchange
Input [1]: [t_time_sk#29]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=5]

(38) BroadcastHashJoin [codegen id : 8]
Left keys [1]: [ss_sold_time_sk#23]
Right keys [1]: [t_time_sk#29]
Join type: Inner
Join condition: None

(39) Project [codegen id : 8]
Output [2]: [ss_hdemo_sk#24, ss_store_sk#25]
Input [4]: [ss_sold_time_sk#23, ss_hdemo_sk#24, ss_store_sk#25, t_time_sk#29]

(40) ReusedExchange [Reuses operator id: 16]
Output [1]: [s_store_sk#32]

(41) BroadcastHashJoin [codegen id : 8]
Left keys [1]: [ss_store_sk#25]
Right keys [1]: [s_store_sk#32]
Join type: Inner
Join condition: None

(42) Project [codegen id : 8]
Output [1]: [ss_hdemo_sk#24]
Input [3]: [ss_hdemo_sk#24, ss_store_sk#25, s_store_sk#32]

(43) ReusedExchange [Reuses operator id: 23]
Output [1]: [hd_demo_sk#33]

(44) BroadcastHashJoin [codegen id : 8]
Left keys [1]: [ss_hdemo_sk#24]
Right keys [1]: [hd_demo_sk#33]
Join type: Inner
Join condition: None

(45) Project [codegen id : 8]
Output: []
Input [2]: [ss_hdemo_sk#24, hd_demo_sk#33]

(46) HashAggregate [codegen id : 8]
Input: []
Keys: []
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#34]
Results [1]: [count#35]

(47) Exchange
Input [1]: [count#35]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=6]

(48) HashAggregate [codegen id : 9]
Input [1]: [count#35]
Keys: []
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#36]
Results [1]: [count(1)#36 AS h9_to_9_30#37]

(49) BroadcastExchange
Input [1]: [h9_to_9_30#37]
Arguments: IdentityBroadcastMode, [plan_id=7]

(50) BroadcastNestedLoopJoin [codegen id : 40]
Join type: Inner
Join condition: None

(51) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_sold_time_sk#38, ss_hdemo_sk#39, ss_store_sk#40, ss_sold_date_sk#41]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>

(52) ColumnarToRow [codegen id : 13]
Input [4]: [ss_sold_time_sk#38, ss_hdemo_sk#39, ss_store_sk#40, ss_sold_date_sk#41]

(53) Filter [codegen id : 13]
Input [4]: [ss_sold_time_sk#38, ss_hdemo_sk#39, ss_store_sk#40, ss_sold_date_sk#41]
Condition : (((((isnotnull(ss_hdemo_sk#39) AND isnotnull(ss_sold_time_sk#38)) AND isnotnull(ss_store_sk#40)) AND might_contain(runtimefilterexpression(Subquery scalar-subquery#42, [id=#43]), xxhash64(ss_sold_time_sk#38, 42))) AND might_contain(runtimefilterexpression(ReusedSubquery Subquery scalar-subquery#7, [id=#8]), xxhash64(ss_store_sk#40, 42))) AND might_contain(runtimefilterexpression(ReusedSubquery Subquery scalar-subquery#9, [id=#10]), xxhash64(ss_hdemo_sk#39, 42)))

(54) Project [codegen id : 13]
Output [3]: [ss_sold_time_sk#38, ss_hdemo_sk#39, ss_store_sk#40]
Input [4]: [ss_sold_time_sk#38, ss_hdemo_sk#39, ss_store_sk#40, ss_sold_date_sk#41]

(55) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#44, t_hour#45, t_minute#46]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,9), GreaterThanOrEqual(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(56) ColumnarToRow [codegen id : 10]
Input [3]: [t_time_sk#44, t_hour#45, t_minute#46]

(57) Filter [codegen id : 10]
Input [3]: [t_time_sk#44, t_hour#45, t_minute#46]
Condition : ((((isnotnull(t_hour#45) AND isnotnull(t_minute#46)) AND (t_hour#45 = 9)) AND (t_minute#46 >= 30)) AND isnotnull(t_time_sk#44))

(58) Project [codegen id : 10]
Output [1]: [t_time_sk#44]
Input [3]: [t_time_sk#44, t_hour#45, t_minute#46]

(59) BroadcastExchange
Input [1]: [t_time_sk#44]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=8]

(60) BroadcastHashJoin [codegen id : 13]
Left keys [1]: [ss_sold_time_sk#38]
Right keys [1]: [t_time_sk#44]
Join type: Inner
Join condition: None

(61) Project [codegen id : 13]
Output [2]: [ss_hdemo_sk#39, ss_store_sk#40]
Input [4]: [ss_sold_time_sk#38, ss_hdemo_sk#39, ss_store_sk#40, t_time_sk#44]

(62) ReusedExchange [Reuses operator id: 16]
Output [1]: [s_store_sk#47]

(63) BroadcastHashJoin [codegen id : 13]
Left keys [1]: [ss_store_sk#40]
Right keys [1]: [s_store_sk#47]
Join type: Inner
Join condition: None

(64) Project [codegen id : 13]
Output [1]: [ss_hdemo_sk#39]
Input [3]: [ss_hdemo_sk#39, ss_store_sk#40, s_store_sk#47]

(65) ReusedExchange [Reuses operator id: 23]
Output [1]: [hd_demo_sk#48]

(66) BroadcastHashJoin [codegen id : 13]
Left keys [1]: [ss_hdemo_sk#39]
Right keys [1]: [hd_demo_sk#48]
Join type: Inner
Join condition: None

(67) Project [codegen id : 13]
Output: []
Input [2]: [ss_hdemo_sk#39, hd_demo_sk#48]

(68) HashAggregate [codegen id : 13]
Input: []
Keys: []
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#49]
Results [1]: [count#50]

(69) Exchange
Input [1]: [count#50]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=9]

(70) HashAggregate [codegen id : 14]
Input [1]: [count#50]
Keys: []
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#51]
Results [1]: [count(1)#51 AS h9_30_to_10#52]

(71) BroadcastExchange
Input [1]: [h9_30_to_10#52]
Arguments: IdentityBroadcastMode, [plan_id=10]

(72) BroadcastNestedLoopJoin [codegen id : 40]
Join type: Inner
Join condition: None

(73) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_sold_time_sk#53, ss_hdemo_sk#54, ss_store_sk#55, ss_sold_date_sk#56]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>

(74) ColumnarToRow [codegen id : 18]
Input [4]: [ss_sold_time_sk#53, ss_hdemo_sk#54, ss_store_sk#55, ss_sold_date_sk#56]

(75) Filter [codegen id : 18]
Input [4]: [ss_sold_time_sk#53, ss_hdemo_sk#54, ss_store_sk#55, ss_sold_date_sk#56]
Condition : (((isnotnull(ss_hdemo_sk#54) AND isnotnull(ss_sold_time_sk#53)) AND isnotnull(ss_store_sk#55)) AND might_contain(runtimefilterexpression(Subquery scalar-subquery#57, [id=#58]), xxhash64(ss_sold_time_sk#53, 42)))

(76) Project [codegen id : 18]
Output [3]: [ss_sold_time_sk#53, ss_hdemo_sk#54, ss_store_sk#55]
Input [4]: [ss_sold_time_sk#53, ss_hdemo_sk#54, ss_store_sk#55, ss_sold_date_sk#56]

(77) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#59, t_hour#60, t_minute#61]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,10), LessThan(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(78) ColumnarToRow [codegen id : 15]
Input [3]: [t_time_sk#59, t_hour#60, t_minute#61]

(79) Filter [codegen id : 15]
Input [3]: [t_time_sk#59, t_hour#60, t_minute#61]
Condition : ((((isnotnull(t_hour#60) AND isnotnull(t_minute#61)) AND (t_hour#60 = 10)) AND (t_minute#61 < 30)) AND isnotnull(t_time_sk#59))

(80) Project [codegen id : 15]
Output [1]: [t_time_sk#59]
Input [3]: [t_time_sk#59, t_hour#60, t_minute#61]

(81) BroadcastExchange
Input [1]: [t_time_sk#59]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=11]

(82) BroadcastHashJoin [codegen id : 18]
Left keys [1]: [ss_sold_time_sk#53]
Right keys [1]: [t_time_sk#59]
Join type: Inner
Join condition: None

(83) Project [codegen id : 18]
Output [2]: [ss_hdemo_sk#54, ss_store_sk#55]
Input [4]: [ss_sold_time_sk#53, ss_hdemo_sk#54, ss_store_sk#55, t_time_sk#59]

(84) ReusedExchange [Reuses operator id: 16]
Output [1]: [s_store_sk#62]

(85) BroadcastHashJoin [codegen id : 18]
Left keys [1]: [ss_store_sk#55]
Right keys [1]: [s_store_sk#62]
Join type: Inner
Join condition: None

(86) Project [codegen id : 18]
Output [1]: [ss_hdemo_sk#54]
Input [3]: [ss_hdemo_sk#54, ss_store_sk#55, s_store_sk#62]

(87) ReusedExchange [Reuses operator id: 23]
Output [1]: [hd_demo_sk#63]

(88) BroadcastHashJoin [codegen id : 18]
Left keys [1]: [ss_hdemo_sk#54]
Right keys [1]: [hd_demo_sk#63]
Join type: Inner
Join condition: None

(89) Project [codegen id : 18]
Output: []
Input [2]: [ss_hdemo_sk#54, hd_demo_sk#63]

(90) HashAggregate [codegen id : 18]
Input: []
Keys: []
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#64]
Results [1]: [count#65]

(91) Exchange
Input [1]: [count#65]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=12]

(92) HashAggregate [codegen id : 19]
Input [1]: [count#65]
Keys: []
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#66]
Results [1]: [count(1)#66 AS h10_to_10_30#67]

(93) BroadcastExchange
Input [1]: [h10_to_10_30#67]
Arguments: IdentityBroadcastMode, [plan_id=13]

(94) BroadcastNestedLoopJoin [codegen id : 40]
Join type: Inner
Join condition: None

(95) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_sold_time_sk#68, ss_hdemo_sk#69, ss_store_sk#70, ss_sold_date_sk#71]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>

(96) ColumnarToRow [codegen id : 23]
Input [4]: [ss_sold_time_sk#68, ss_hdemo_sk#69, ss_store_sk#70, ss_sold_date_sk#71]

(97) Filter [codegen id : 23]
Input [4]: [ss_sold_time_sk#68, ss_hdemo_sk#69, ss_store_sk#70, ss_sold_date_sk#71]
Condition : ((isnotnull(ss_hdemo_sk#69) AND isnotnull(ss_sold_time_sk#68)) AND isnotnull(ss_store_sk#70))

(98) Project [codegen id : 23]
Output [3]: [ss_sold_time_sk#68, ss_hdemo_sk#69, ss_store_sk#70]
Input [4]: [ss_sold_time_sk#68, ss_hdemo_sk#69, ss_store_sk#70, ss_sold_date_sk#71]

(99) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#72, t_hour#73, t_minute#74]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,10), GreaterThanOrEqual(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(100) ColumnarToRow [codegen id : 20]
Input [3]: [t_time_sk#72, t_hour#73, t_minute#74]

(101) Filter [codegen id : 20]
Input [3]: [t_time_sk#72, t_hour#73, t_minute#74]
Condition : ((((isnotnull(t_hour#73) AND isnotnull(t_minute#74)) AND (t_hour#73 = 10)) AND (t_minute#74 >= 30)) AND isnotnull(t_time_sk#72))

(102) Project [codegen id : 20]
Output [1]: [t_time_sk#72]
Input [3]: [t_time_sk#72, t_hour#73, t_minute#74]

(103) BroadcastExchange
Input [1]: [t_time_sk#72]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=14]

(104) BroadcastHashJoin [codegen id : 23]
Left keys [1]: [ss_sold_time_sk#68]
Right keys [1]: [t_time_sk#72]
Join type: Inner
Join condition: None

(105) Project [codegen id : 23]
Output [2]: [ss_hdemo_sk#69, ss_store_sk#70]
Input [4]: [ss_sold_time_sk#68, ss_hdemo_sk#69, ss_store_sk#70, t_time_sk#72]

(106) ReusedExchange [Reuses operator id: 16]
Output [1]: [s_store_sk#75]

(107) BroadcastHashJoin [codegen id : 23]
Left keys [1]: [ss_store_sk#70]
Right keys [1]: [s_store_sk#75]
Join type: Inner
Join condition: None

(108) Project [codegen id : 23]
Output [1]: [ss_hdemo_sk#69]
Input [3]: [ss_hdemo_sk#69, ss_store_sk#70, s_store_sk#75]

(109) ReusedExchange [Reuses operator id: 23]
Output [1]: [hd_demo_sk#76]

(110) BroadcastHashJoin [codegen id : 23]
Left keys [1]: [ss_hdemo_sk#69]
Right keys [1]: [hd_demo_sk#76]
Join type: Inner
Join condition: None

(111) Project [codegen id : 23]
Output: []
Input [2]: [ss_hdemo_sk#69, hd_demo_sk#76]

(112) HashAggregate [codegen id : 23]
Input: []
Keys: []
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#77]
Results [1]: [count#78]

(113) Exchange
Input [1]: [count#78]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=15]

(114) HashAggregate [codegen id : 24]
Input [1]: [count#78]
Keys: []
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#79]
Results [1]: [count(1)#79 AS h10_30_to_11#80]

(115) BroadcastExchange
Input [1]: [h10_30_to_11#80]
Arguments: IdentityBroadcastMode, [plan_id=16]

(116) BroadcastNestedLoopJoin [codegen id : 40]
Join type: Inner
Join condition: None

(117) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_sold_time_sk#81, ss_hdemo_sk#82, ss_store_sk#83, ss_sold_date_sk#84]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>

(118) ColumnarToRow [codegen id : 28]
Input [4]: [ss_sold_time_sk#81, ss_hdemo_sk#82, ss_store_sk#83, ss_sold_date_sk#84]

(119) Filter [codegen id : 28]
Input [4]: [ss_sold_time_sk#81, ss_hdemo_sk#82, ss_store_sk#83, ss_sold_date_sk#84]
Condition : ((isnotnull(ss_hdemo_sk#82) AND isnotnull(ss_sold_time_sk#81)) AND isnotnull(ss_store_sk#83))

(120) Project [codegen id : 28]
Output [3]: [ss_sold_time_sk#81, ss_hdemo_sk#82, ss_store_sk#83]
Input [4]: [ss_sold_time_sk#81, ss_hdemo_sk#82, ss_store_sk#83, ss_sold_date_sk#84]

(121) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#85, t_hour#86, t_minute#87]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,11), LessThan(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(122) ColumnarToRow [codegen id : 25]
Input [3]: [t_time_sk#85, t_hour#86, t_minute#87]

(123) Filter [codegen id : 25]
Input [3]: [t_time_sk#85, t_hour#86, t_minute#87]
Condition : ((((isnotnull(t_hour#86) AND isnotnull(t_minute#87)) AND (t_hour#86 = 11)) AND (t_minute#87 < 30)) AND isnotnull(t_time_sk#85))

(124) Project [codegen id : 25]
Output [1]: [t_time_sk#85]
Input [3]: [t_time_sk#85, t_hour#86, t_minute#87]

(125) BroadcastExchange
Input [1]: [t_time_sk#85]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=17]

(126) BroadcastHashJoin [codegen id : 28]
Left keys [1]: [ss_sold_time_sk#81]
Right keys [1]: [t_time_sk#85]
Join type: Inner
Join condition: None

(127) Project [codegen id : 28]
Output [2]: [ss_hdemo_sk#82, ss_store_sk#83]
Input [4]: [ss_sold_time_sk#81, ss_hdemo_sk#82, ss_store_sk#83, t_time_sk#85]

(128) ReusedExchange [Reuses operator id: 16]
Output [1]: [s_store_sk#88]

(129) BroadcastHashJoin [codegen id : 28]
Left keys [1]: [ss_store_sk#83]
Right keys [1]: [s_store_sk#88]
Join type: Inner
Join condition: None

(130) Project [codegen id : 28]
Output [1]: [ss_hdemo_sk#82]
Input [3]: [ss_hdemo_sk#82, ss_store_sk#83, s_store_sk#88]

(131) ReusedExchange [Reuses operator id: 23]
Output [1]: [hd_demo_sk#89]

(132) BroadcastHashJoin [codegen id : 28]
Left keys [1]: [ss_hdemo_sk#82]
Right keys [1]: [hd_demo_sk#89]
Join type: Inner
Join condition: None

(133) Project [codegen id : 28]
Output: []
Input [2]: [ss_hdemo_sk#82, hd_demo_sk#89]

(134) HashAggregate [codegen id : 28]
Input: []
Keys: []
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#90]
Results [1]: [count#91]

(135) Exchange
Input [1]: [count#91]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=18]

(136) HashAggregate [codegen id : 29]
Input [1]: [count#91]
Keys: []
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#92]
Results [1]: [count(1)#92 AS h11_to_11_30#93]

(137) BroadcastExchange
Input [1]: [h11_to_11_30#93]
Arguments: IdentityBroadcastMode, [plan_id=19]

(138) BroadcastNestedLoopJoin [codegen id : 40]
Join type: Inner
Join condition: None

(139) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_sold_time_sk#94, ss_hdemo_sk#95, ss_store_sk#96, ss_sold_date_sk#97]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>

(140) ColumnarToRow [codegen id : 33]
Input [4]: [ss_sold_time_sk#94, ss_hdemo_sk#95, ss_store_sk#96, ss_sold_date_sk#97]

(141) Filter [codegen id : 33]
Input [4]: [ss_sold_time_sk#94, ss_hdemo_sk#95, ss_store_sk#96, ss_sold_date_sk#97]
Condition : ((isnotnull(ss_hdemo_sk#95) AND isnotnull(ss_sold_time_sk#94)) AND isnotnull(ss_store_sk#96))

(142) Project [codegen id : 33]
Output [3]: [ss_sold_time_sk#94, ss_hdemo_sk#95, ss_store_sk#96]
Input [4]: [ss_sold_time_sk#94, ss_hdemo_sk#95, ss_store_sk#96, ss_sold_date_sk#97]

(143) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#98, t_hour#99, t_minute#100]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,11), GreaterThanOrEqual(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(144) ColumnarToRow [codegen id : 30]
Input [3]: [t_time_sk#98, t_hour#99, t_minute#100]

(145) Filter [codegen id : 30]
Input [3]: [t_time_sk#98, t_hour#99, t_minute#100]
Condition : ((((isnotnull(t_hour#99) AND isnotnull(t_minute#100)) AND (t_hour#99 = 11)) AND (t_minute#100 >= 30)) AND isnotnull(t_time_sk#98))

(146) Project [codegen id : 30]
Output [1]: [t_time_sk#98]
Input [3]: [t_time_sk#98, t_hour#99, t_minute#100]

(147) BroadcastExchange
Input [1]: [t_time_sk#98]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=20]

(148) BroadcastHashJoin [codegen id : 33]
Left keys [1]: [ss_sold_time_sk#94]
Right keys [1]: [t_time_sk#98]
Join type: Inner
Join condition: None

(149) Project [codegen id : 33]
Output [2]: [ss_hdemo_sk#95, ss_store_sk#96]
Input [4]: [ss_sold_time_sk#94, ss_hdemo_sk#95, ss_store_sk#96, t_time_sk#98]

(150) ReusedExchange [Reuses operator id: 16]
Output [1]: [s_store_sk#101]

(151) BroadcastHashJoin [codegen id : 33]
Left keys [1]: [ss_store_sk#96]
Right keys [1]: [s_store_sk#101]
Join type: Inner
Join condition: None

(152) Project [codegen id : 33]
Output [1]: [ss_hdemo_sk#95]
Input [3]: [ss_hdemo_sk#95, ss_store_sk#96, s_store_sk#101]

(153) ReusedExchange [Reuses operator id: 23]
Output [1]: [hd_demo_sk#102]

(154) BroadcastHashJoin [codegen id : 33]
Left keys [1]: [ss_hdemo_sk#95]
Right keys [1]: [hd_demo_sk#102]
Join type: Inner
Join condition: None

(155) Project [codegen id : 33]
Output: []
Input [2]: [ss_hdemo_sk#95, hd_demo_sk#102]

(156) HashAggregate [codegen id : 33]
Input: []
Keys: []
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#103]
Results [1]: [count#104]

(157) Exchange
Input [1]: [count#104]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=21]

(158) HashAggregate [codegen id : 34]
Input [1]: [count#104]
Keys: []
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#105]
Results [1]: [count(1)#105 AS h11_30_to_12#106]

(159) BroadcastExchange
Input [1]: [h11_30_to_12#106]
Arguments: IdentityBroadcastMode, [plan_id=22]

(160) BroadcastNestedLoopJoin [codegen id : 40]
Join type: Inner
Join condition: None

(161) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_sold_time_sk#107, ss_hdemo_sk#108, ss_store_sk#109, ss_sold_date_sk#110]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>

(162) ColumnarToRow [codegen id : 38]
Input [4]: [ss_sold_time_sk#107, ss_hdemo_sk#108, ss_store_sk#109, ss_sold_date_sk#110]

(163) Filter [codegen id : 38]
Input [4]: [ss_sold_time_sk#107, ss_hdemo_sk#108, ss_store_sk#109, ss_sold_date_sk#110]
Condition : ((isnotnull(ss_hdemo_sk#108) AND isnotnull(ss_sold_time_sk#107)) AND isnotnull(ss_store_sk#109))

(164) Project [codegen id : 38]
Output [3]: [ss_sold_time_sk#107, ss_hdemo_sk#108, ss_store_sk#109]
Input [4]: [ss_sold_time_sk#107, ss_hdemo_sk#108, ss_store_sk#109, ss_sold_date_sk#110]

(165) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#111, t_hour#112, t_minute#113]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,12), LessThan(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(166) ColumnarToRow [codegen id : 35]
Input [3]: [t_time_sk#111, t_hour#112, t_minute#113]

(167) Filter [codegen id : 35]
Input [3]: [t_time_sk#111, t_hour#112, t_minute#113]
Condition : ((((isnotnull(t_hour#112) AND isnotnull(t_minute#113)) AND (t_hour#112 = 12)) AND (t_minute#113 < 30)) AND isnotnull(t_time_sk#111))

(168) Project [codegen id : 35]
Output [1]: [t_time_sk#111]
Input [3]: [t_time_sk#111, t_hour#112, t_minute#113]

(169) BroadcastExchange
Input [1]: [t_time_sk#111]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=23]

(170) BroadcastHashJoin [codegen id : 38]
Left keys [1]: [ss_sold_time_sk#107]
Right keys [1]: [t_time_sk#111]
Join type: Inner
Join condition: None

(171) Project [codegen id : 38]
Output [2]: [ss_hdemo_sk#108, ss_store_sk#109]
Input [4]: [ss_sold_time_sk#107, ss_hdemo_sk#108, ss_store_sk#109, t_time_sk#111]

(172) ReusedExchange [Reuses operator id: 16]
Output [1]: [s_store_sk#114]

(173) BroadcastHashJoin [codegen id : 38]
Left keys [1]: [ss_store_sk#109]
Right keys [1]: [s_store_sk#114]
Join type: Inner
Join condition: None

(174) Project [codegen id : 38]
Output [1]: [ss_hdemo_sk#108]
Input [3]: [ss_hdemo_sk#108, ss_store_sk#109, s_store_sk#114]

(175) ReusedExchange [Reuses operator id: 23]
Output [1]: [hd_demo_sk#115]

(176) BroadcastHashJoin [codegen id : 38]
Left keys [1]: [ss_hdemo_sk#108]
Right keys [1]: [hd_demo_sk#115]
Join type: Inner
Join condition: None

(177) Project [codegen id : 38]
Output: []
Input [2]: [ss_hdemo_sk#108, hd_demo_sk#115]

(178) HashAggregate [codegen id : 38]
Input: []
Keys: []
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#116]
Results [1]: [count#117]

(179) Exchange
Input [1]: [count#117]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=24]

(180) HashAggregate [codegen id : 39]
Input [1]: [count#117]
Keys: []
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#118]
Results [1]: [count(1)#118 AS h12_to_12_30#119]

(181) BroadcastExchange
Input [1]: [h12_to_12_30#119]
Arguments: IdentityBroadcastMode, [plan_id=25]

(182) BroadcastNestedLoopJoin [codegen id : 40]
Join type: Inner
Join condition: None

===== Subqueries =====

Subquery:1 Hosting operator id = 3 Hosting Expression = Subquery scalar-subquery#5, [id=#6]
ObjectHashAggregate (189)
+- Exchange (188)
   +- ObjectHashAggregate (187)
      +- * Project (186)
         +- * Filter (185)
            +- * ColumnarToRow (184)
               +- Scan parquet spark_catalog.default.time_dim (183)


(183) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#11, t_hour#12, t_minute#13]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,8), GreaterThanOrEqual(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(184) ColumnarToRow [codegen id : 1]
Input [3]: [t_time_sk#11, t_hour#12, t_minute#13]

(185) Filter [codegen id : 1]
Input [3]: [t_time_sk#11, t_hour#12, t_minute#13]
Condition : ((((isnotnull(t_hour#12) AND isnotnull(t_minute#13)) AND (t_hour#12 = 8)) AND (t_minute#13 >= 30)) AND isnotnull(t_time_sk#11))

(186) Project [codegen id : 1]
Output [1]: [t_time_sk#11]
Input [3]: [t_time_sk#11, t_hour#12, t_minute#13]

(187) ObjectHashAggregate
Input [1]: [t_time_sk#11]
Keys: []
Functions [1]: [partial_bloom_filter_agg(xxhash64(t_time_sk#11, 42), 1699, 13592, 0, 0)]
Aggregate Attributes [1]: [buf#120]
Results [1]: [buf#121]

(188) Exchange
Input [1]: [buf#121]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=26]

(189) ObjectHashAggregate
Input [1]: [buf#121]
Keys: []
Functions [1]: [bloom_filter_agg(xxhash64(t_time_sk#11, 42), 1699, 13592, 0, 0)]
Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(t_time_sk#11, 42), 1699, 13592, 0, 0)#122]
Results [1]: [bloom_filter_agg(xxhash64(t_time_sk#11, 42), 1699, 13592, 0, 0)#122 AS bloomFilter#123]

Subquery:2 Hosting operator id = 3 Hosting Expression = Subquery scalar-subquery#7, [id=#8]
ObjectHashAggregate (196)
+- Exchange (195)
   +- ObjectHashAggregate (194)
      +- * Project (193)
         +- * Filter (192)
            +- * ColumnarToRow (191)
               +- Scan parquet spark_catalog.default.store (190)


(190) Scan parquet spark_catalog.default.store
Output [2]: [s_store_sk#14, s_store_name#15]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store]
PushedFilters: [IsNotNull(s_store_name), EqualTo(s_store_name,ese), IsNotNull(s_store_sk)]
ReadSchema: struct<s_store_sk:int,s_store_name:string>

(191) ColumnarToRow [codegen id : 1]
Input [2]: [s_store_sk#14, s_store_name#15]

(192) Filter [codegen id : 1]
Input [2]: [s_store_sk#14, s_store_name#15]
Condition : ((isnotnull(s_store_name#15) AND (s_store_name#15 = ese)) AND isnotnull(s_store_sk#14))

(193) Project [codegen id : 1]
Output [1]: [s_store_sk#14]
Input [2]: [s_store_sk#14, s_store_name#15]

(194) ObjectHashAggregate
Input [1]: [s_store_sk#14]
Keys: []
Functions [1]: [partial_bloom_filter_agg(xxhash64(s_store_sk#14, 42), 41, 328, 0, 0)]
Aggregate Attributes [1]: [buf#124]
Results [1]: [buf#125]

(195) Exchange
Input [1]: [buf#125]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=27]

(196) ObjectHashAggregate
Input [1]: [buf#125]
Keys: []
Functions [1]: [bloom_filter_agg(xxhash64(s_store_sk#14, 42), 41, 328, 0, 0)]
Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(s_store_sk#14, 42), 41, 328, 0, 0)#126]
Results [1]: [bloom_filter_agg(xxhash64(s_store_sk#14, 42), 41, 328, 0, 0)#126 AS bloomFilter#127]

Subquery:3 Hosting operator id = 3 Hosting Expression = Subquery scalar-subquery#9, [id=#10]
ObjectHashAggregate (203)
+- Exchange (202)
   +- ObjectHashAggregate (201)
      +- * Project (200)
         +- * Filter (199)
            +- * ColumnarToRow (198)
               +- Scan parquet spark_catalog.default.household_demographics (197)


(197) Scan parquet spark_catalog.default.household_demographics
Output [3]: [hd_demo_sk#16, hd_dep_count#17, hd_vehicle_count#18]
Batched: true
Location [not included in comparison]/{warehouse_dir}/household_demographics]
PushedFilters: [Or(Or(And(EqualTo(hd_dep_count,4),LessThanOrEqual(hd_vehicle_count,6)),And(EqualTo(hd_dep_count,2),LessThanOrEqual(hd_vehicle_count,4))),And(EqualTo(hd_dep_count,0),LessThanOrEqual(hd_vehicle_count,2))), IsNotNull(hd_demo_sk)]
ReadSchema: struct<hd_demo_sk:int,hd_dep_count:int,hd_vehicle_count:int>

(198) ColumnarToRow [codegen id : 1]
Input [3]: [hd_demo_sk#16, hd_dep_count#17, hd_vehicle_count#18]

(199) Filter [codegen id : 1]
Input [3]: [hd_demo_sk#16, hd_dep_count#17, hd_vehicle_count#18]
Condition : (((((hd_dep_count#17 = 4) AND (hd_vehicle_count#18 <= 6)) OR ((hd_dep_count#17 = 2) AND (hd_vehicle_count#18 <= 4))) OR ((hd_dep_count#17 = 0) AND (hd_vehicle_count#18 <= 2))) AND isnotnull(hd_demo_sk#16))

(200) Project [codegen id : 1]
Output [1]: [hd_demo_sk#16]
Input [3]: [hd_demo_sk#16, hd_dep_count#17, hd_vehicle_count#18]

(201) ObjectHashAggregate
Input [1]: [hd_demo_sk#16]
Keys: []
Functions [1]: [partial_bloom_filter_agg(xxhash64(hd_demo_sk#16, 42), 1718, 13744, 0, 0)]
Aggregate Attributes [1]: [buf#128]
Results [1]: [buf#129]

(202) Exchange
Input [1]: [buf#129]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=28]

(203) ObjectHashAggregate
Input [1]: [buf#129]
Keys: []
Functions [1]: [bloom_filter_agg(xxhash64(hd_demo_sk#16, 42), 1718, 13744, 0, 0)]
Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(hd_demo_sk#16, 42), 1718, 13744, 0, 0)#130]
Results [1]: [bloom_filter_agg(xxhash64(hd_demo_sk#16, 42), 1718, 13744, 0, 0)#130 AS bloomFilter#131]

Subquery:4 Hosting operator id = 31 Hosting Expression = Subquery scalar-subquery#27, [id=#28]
ObjectHashAggregate (210)
+- Exchange (209)
   +- ObjectHashAggregate (208)
      +- * Project (207)
         +- * Filter (206)
            +- * ColumnarToRow (205)
               +- Scan parquet spark_catalog.default.time_dim (204)


(204) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#29, t_hour#30, t_minute#31]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,9), LessThan(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(205) ColumnarToRow [codegen id : 1]
Input [3]: [t_time_sk#29, t_hour#30, t_minute#31]

(206) Filter [codegen id : 1]
Input [3]: [t_time_sk#29, t_hour#30, t_minute#31]
Condition : ((((isnotnull(t_hour#30) AND isnotnull(t_minute#31)) AND (t_hour#30 = 9)) AND (t_minute#31 < 30)) AND isnotnull(t_time_sk#29))

(207) Project [codegen id : 1]
Output [1]: [t_time_sk#29]
Input [3]: [t_time_sk#29, t_hour#30, t_minute#31]

(208) ObjectHashAggregate
Input [1]: [t_time_sk#29]
Keys: []
Functions [1]: [partial_bloom_filter_agg(xxhash64(t_time_sk#29, 42), 1758, 14064, 0, 0)]
Aggregate Attributes [1]: [buf#132]
Results [1]: [buf#133]

(209) Exchange
Input [1]: [buf#133]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=29]

(210) ObjectHashAggregate
Input [1]: [buf#133]
Keys: []
Functions [1]: [bloom_filter_agg(xxhash64(t_time_sk#29, 42), 1758, 14064, 0, 0)]
Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(t_time_sk#29, 42), 1758, 14064, 0, 0)#134]
Results [1]: [bloom_filter_agg(xxhash64(t_time_sk#29, 42), 1758, 14064, 0, 0)#134 AS bloomFilter#135]

Subquery:5 Hosting operator id = 31 Hosting Expression = ReusedSubquery Subquery scalar-subquery#7, [id=#8]

Subquery:6 Hosting operator id = 31 Hosting Expression = ReusedSubquery Subquery scalar-subquery#9, [id=#10]

Subquery:7 Hosting operator id = 53 Hosting Expression = Subquery scalar-subquery#42, [id=#43]
ObjectHashAggregate (217)
+- Exchange (216)
   +- ObjectHashAggregate (215)
      +- * Project (214)
         +- * Filter (213)
            +- * ColumnarToRow (212)
               +- Scan parquet spark_catalog.default.time_dim (211)


(211) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#44, t_hour#45, t_minute#46]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,9), GreaterThanOrEqual(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(212) ColumnarToRow [codegen id : 1]
Input [3]: [t_time_sk#44, t_hour#45, t_minute#46]

(213) Filter [codegen id : 1]
Input [3]: [t_time_sk#44, t_hour#45, t_minute#46]
Condition : ((((isnotnull(t_hour#45) AND isnotnull(t_minute#46)) AND (t_hour#45 = 9)) AND (t_minute#46 >= 30)) AND isnotnull(t_time_sk#44))

(214) Project [codegen id : 1]
Output [1]: [t_time_sk#44]
Input [3]: [t_time_sk#44, t_hour#45, t_minute#46]

(215) ObjectHashAggregate
Input [1]: [t_time_sk#44]
Keys: []
Functions [1]: [partial_bloom_filter_agg(xxhash64(t_time_sk#44, 42), 1699, 13592, 0, 0)]
Aggregate Attributes [1]: [buf#136]
Results [1]: [buf#137]

(216) Exchange
Input [1]: [buf#137]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=30]

(217) ObjectHashAggregate
Input [1]: [buf#137]
Keys: []
Functions [1]: [bloom_filter_agg(xxhash64(t_time_sk#44, 42), 1699, 13592, 0, 0)]
Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(t_time_sk#44, 42), 1699, 13592, 0, 0)#138]
Results [1]: [bloom_filter_agg(xxhash64(t_time_sk#44, 42), 1699, 13592, 0, 0)#138 AS bloomFilter#139]

Subquery:8 Hosting operator id = 53 Hosting Expression = ReusedSubquery Subquery scalar-subquery#7, [id=#8]

Subquery:9 Hosting operator id = 53 Hosting Expression = ReusedSubquery Subquery scalar-subquery#9, [id=#10]

Subquery:10 Hosting operator id = 75 Hosting Expression = Subquery scalar-subquery#57, [id=#58]
ObjectHashAggregate (224)
+- Exchange (223)
   +- ObjectHashAggregate (222)
      +- * Project (221)
         +- * Filter (220)
            +- * ColumnarToRow (219)
               +- Scan parquet spark_catalog.default.time_dim (218)


(218) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#59, t_hour#60, t_minute#61]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,10), LessThan(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(219) ColumnarToRow [codegen id : 1]
Input [3]: [t_time_sk#59, t_hour#60, t_minute#61]

(220) Filter [codegen id : 1]
Input [3]: [t_time_sk#59, t_hour#60, t_minute#61]
Condition : ((((isnotnull(t_hour#60) AND isnotnull(t_minute#61)) AND (t_hour#60 = 10)) AND (t_minute#61 < 30)) AND isnotnull(t_time_sk#59))

(221) Project [codegen id : 1]
Output [1]: [t_time_sk#59]
Input [3]: [t_time_sk#59, t_hour#60, t_minute#61]

(222) ObjectHashAggregate
Input [1]: [t_time_sk#59]
Keys: []
Functions [1]: [partial_bloom_filter_agg(xxhash64(t_time_sk#59, 42), 1758, 14064, 0, 0)]
Aggregate Attributes [1]: [buf#140]
Results [1]: [buf#141]

(223) Exchange
Input [1]: [buf#141]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=31]

(224) ObjectHashAggregate
Input [1]: [buf#141]
Keys: []
Functions [1]: [bloom_filter_agg(xxhash64(t_time_sk#59, 42), 1758, 14064, 0, 0)]
Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(t_time_sk#59, 42), 1758, 14064, 0, 0)#142]
Results [1]: [bloom_filter_agg(xxhash64(t_time_sk#59, 42), 1758, 14064, 0, 0)#142 AS bloomFilter#143]


