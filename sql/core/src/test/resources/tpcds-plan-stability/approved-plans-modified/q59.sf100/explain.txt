== Physical Plan ==
TakeOrderedAndProject (46)
+- * Project (45)
   +- * BroadcastHashJoin Inner BuildRight (44)
      :- * Project (21)
      :  +- * BroadcastHashJoin Inner BuildRight (20)
      :     :- * Project (18)
      :     :  +- * BroadcastHashJoin Inner BuildRight (17)
      :     :     :- * HashAggregate (12)
      :     :     :  +- Exchange (11)
      :     :     :     +- * HashAggregate (10)
      :     :     :        +- * Project (9)
      :     :     :           +- * BroadcastHashJoin Inner BuildRight (8)
      :     :     :              :- * Filter (3)
      :     :     :              :  +- * ColumnarToRow (2)
      :     :     :              :     +- Scan parquet spark_catalog.default.store_sales (1)
      :     :     :              +- BroadcastExchange (7)
      :     :     :                 +- * Filter (6)
      :     :     :                    +- * ColumnarToRow (5)
      :     :     :                       +- Scan parquet spark_catalog.default.date_dim (4)
      :     :     +- BroadcastExchange (16)
      :     :        +- * Filter (15)
      :     :           +- * ColumnarToRow (14)
      :     :              +- Scan parquet spark_catalog.default.store (13)
      :     +- ReusedExchange (19)
      +- BroadcastExchange (43)
         +- * Project (42)
            +- * BroadcastHashJoin Inner BuildRight (41)
               :- * Project (39)
               :  +- * BroadcastHashJoin Inner BuildRight (38)
               :     :- * HashAggregate (33)
               :     :  +- Exchange (32)
               :     :     +- * HashAggregate (31)
               :     :        +- * Project (30)
               :     :           +- * BroadcastHashJoin Inner BuildRight (29)
               :     :              :- * Filter (24)
               :     :              :  +- * ColumnarToRow (23)
               :     :              :     +- Scan parquet spark_catalog.default.store_sales (22)
               :     :              +- BroadcastExchange (28)
               :     :                 +- * Filter (27)
               :     :                    +- * ColumnarToRow (26)
               :     :                       +- Scan parquet spark_catalog.default.date_dim (25)
               :     +- BroadcastExchange (37)
               :        +- * Filter (36)
               :           +- * ColumnarToRow (35)
               :              +- Scan parquet spark_catalog.default.store (34)
               +- ReusedExchange (40)


(1) Scan parquet spark_catalog.default.store_sales
Output [3]: [ss_store_sk#1, ss_sales_price#2, ss_sold_date_sk#3]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ss_sold_date_sk#3)]
PushedFilters: [IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_store_sk:int,ss_sales_price:decimal(7,2)>

(2) ColumnarToRow [codegen id : 2]
Input [3]: [ss_store_sk#1, ss_sales_price#2, ss_sold_date_sk#3]

(3) Filter [codegen id : 2]
Input [3]: [ss_store_sk#1, ss_sales_price#2, ss_sold_date_sk#3]
Condition : isnotnull(ss_store_sk#1)

(4) Scan parquet spark_catalog.default.date_dim
Output [3]: [d_date_sk#4, d_week_seq#5, d_day_name#6]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_date_sk), IsNotNull(d_week_seq)]
ReadSchema: struct<d_date_sk:int,d_week_seq:int,d_day_name:string>

(5) ColumnarToRow [codegen id : 1]
Input [3]: [d_date_sk#4, d_week_seq#5, d_day_name#6]

(6) Filter [codegen id : 1]
Input [3]: [d_date_sk#4, d_week_seq#5, d_day_name#6]
Condition : ((isnotnull(d_date_sk#4) AND isnotnull(d_week_seq#5)) AND dynamicpruningexpression(d_week_seq#5 IN dynamicpruning#7))

(7) BroadcastExchange
Input [3]: [d_date_sk#4, d_week_seq#5, d_day_name#6]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1]

(8) BroadcastHashJoin [codegen id : 2]
Left keys [1]: [ss_sold_date_sk#3]
Right keys [1]: [d_date_sk#4]
Join type: Inner
Join condition: None

(9) Project [codegen id : 2]
Output [4]: [ss_store_sk#1, ss_sales_price#2, d_week_seq#5, d_day_name#6]
Input [6]: [ss_store_sk#1, ss_sales_price#2, ss_sold_date_sk#3, d_date_sk#4, d_week_seq#5, d_day_name#6]

(10) HashAggregate [codegen id : 2]
Input [4]: [ss_store_sk#1, ss_sales_price#2, d_week_seq#5, d_day_name#6]
Keys [2]: [d_week_seq#5, ss_store_sk#1]
Functions [7]: [partial_sum(UnscaledValue(CASE WHEN (d_day_name#6 = Sunday   ) THEN ss_sales_price#2 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#6 = Monday   ) THEN ss_sales_price#2 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#6 = Tuesday  ) THEN ss_sales_price#2 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#6 = Wednesday) THEN ss_sales_price#2 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#6 = Thursday ) THEN ss_sales_price#2 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#6 = Friday   ) THEN ss_sales_price#2 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#6 = Saturday ) THEN ss_sales_price#2 END))]
Aggregate Attributes [7]: [sum#8, sum#9, sum#10, sum#11, sum#12, sum#13, sum#14]
Results [9]: [d_week_seq#5, ss_store_sk#1, sum#15, sum#16, sum#17, sum#18, sum#19, sum#20, sum#21]

(11) Exchange
Input [9]: [d_week_seq#5, ss_store_sk#1, sum#15, sum#16, sum#17, sum#18, sum#19, sum#20, sum#21]
Arguments: hashpartitioning(d_week_seq#5, ss_store_sk#1, 5), ENSURE_REQUIREMENTS, [plan_id=2]

(12) HashAggregate [codegen id : 10]
Input [9]: [d_week_seq#5, ss_store_sk#1, sum#15, sum#16, sum#17, sum#18, sum#19, sum#20, sum#21]
Keys [2]: [d_week_seq#5, ss_store_sk#1]
Functions [7]: [sum(UnscaledValue(CASE WHEN (d_day_name#6 = Sunday   ) THEN ss_sales_price#2 END)), sum(UnscaledValue(CASE WHEN (d_day_name#6 = Monday   ) THEN ss_sales_price#2 END)), sum(UnscaledValue(CASE WHEN (d_day_name#6 = Tuesday  ) THEN ss_sales_price#2 END)), sum(UnscaledValue(CASE WHEN (d_day_name#6 = Wednesday) THEN ss_sales_price#2 END)), sum(UnscaledValue(CASE WHEN (d_day_name#6 = Thursday ) THEN ss_sales_price#2 END)), sum(UnscaledValue(CASE WHEN (d_day_name#6 = Friday   ) THEN ss_sales_price#2 END)), sum(UnscaledValue(CASE WHEN (d_day_name#6 = Saturday ) THEN ss_sales_price#2 END))]
Aggregate Attributes [7]: [sum(UnscaledValue(CASE WHEN (d_day_name#6 = Sunday   ) THEN ss_sales_price#2 END))#22, sum(UnscaledValue(CASE WHEN (d_day_name#6 = Monday   ) THEN ss_sales_price#2 END))#23, sum(UnscaledValue(CASE WHEN (d_day_name#6 = Tuesday  ) THEN ss_sales_price#2 END))#24, sum(UnscaledValue(CASE WHEN (d_day_name#6 = Wednesday) THEN ss_sales_price#2 END))#25, sum(UnscaledValue(CASE WHEN (d_day_name#6 = Thursday ) THEN ss_sales_price#2 END))#26, sum(UnscaledValue(CASE WHEN (d_day_name#6 = Friday   ) THEN ss_sales_price#2 END))#27, sum(UnscaledValue(CASE WHEN (d_day_name#6 = Saturday ) THEN ss_sales_price#2 END))#28]
Results [9]: [d_week_seq#5, ss_store_sk#1, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#6 = Sunday   ) THEN ss_sales_price#2 END))#22,17,2) AS sun_sales#29, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#6 = Monday   ) THEN ss_sales_price#2 END))#23,17,2) AS mon_sales#30, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#6 = Tuesday  ) THEN ss_sales_price#2 END))#24,17,2) AS tue_sales#31, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#6 = Wednesday) THEN ss_sales_price#2 END))#25,17,2) AS wed_sales#32, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#6 = Thursday ) THEN ss_sales_price#2 END))#26,17,2) AS thu_sales#33, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#6 = Friday   ) THEN ss_sales_price#2 END))#27,17,2) AS fri_sales#34, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#6 = Saturday ) THEN ss_sales_price#2 END))#28,17,2) AS sat_sales#35]

(13) Scan parquet spark_catalog.default.store
Output [3]: [s_store_sk#36, s_store_id#37, s_store_name#38]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store]
PushedFilters: [IsNotNull(s_store_sk), IsNotNull(s_store_id)]
ReadSchema: struct<s_store_sk:int,s_store_id:string,s_store_name:string>

(14) ColumnarToRow [codegen id : 3]
Input [3]: [s_store_sk#36, s_store_id#37, s_store_name#38]

(15) Filter [codegen id : 3]
Input [3]: [s_store_sk#36, s_store_id#37, s_store_name#38]
Condition : (isnotnull(s_store_sk#36) AND isnotnull(s_store_id#37))

(16) BroadcastExchange
Input [3]: [s_store_sk#36, s_store_id#37, s_store_name#38]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=3]

(17) BroadcastHashJoin [codegen id : 10]
Left keys [1]: [ss_store_sk#1]
Right keys [1]: [s_store_sk#36]
Join type: Inner
Join condition: None

(18) Project [codegen id : 10]
Output [10]: [d_week_seq#5, sun_sales#29, mon_sales#30, tue_sales#31, wed_sales#32, thu_sales#33, fri_sales#34, sat_sales#35, s_store_id#37, s_store_name#38]
Input [12]: [d_week_seq#5, ss_store_sk#1, sun_sales#29, mon_sales#30, tue_sales#31, wed_sales#32, thu_sales#33, fri_sales#34, sat_sales#35, s_store_sk#36, s_store_id#37, s_store_name#38]

(19) ReusedExchange [Reuses operator id: 51]
Output [1]: [d_week_seq#39]

(20) BroadcastHashJoin [codegen id : 10]
Left keys [1]: [d_week_seq#5]
Right keys [1]: [d_week_seq#39]
Join type: Inner
Join condition: None

(21) Project [codegen id : 10]
Output [10]: [s_store_name#38 AS s_store_name1#40, d_week_seq#5 AS d_week_seq1#41, s_store_id#37 AS s_store_id1#42, sun_sales#29 AS sun_sales1#43, mon_sales#30 AS mon_sales1#44, tue_sales#31 AS tue_sales1#45, wed_sales#32 AS wed_sales1#46, thu_sales#33 AS thu_sales1#47, fri_sales#34 AS fri_sales1#48, sat_sales#35 AS sat_sales1#49]
Input [11]: [d_week_seq#5, sun_sales#29, mon_sales#30, tue_sales#31, wed_sales#32, thu_sales#33, fri_sales#34, sat_sales#35, s_store_id#37, s_store_name#38, d_week_seq#39]

(22) Scan parquet spark_catalog.default.store_sales
Output [3]: [ss_store_sk#1, ss_sales_price#2, ss_sold_date_sk#3]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ss_sold_date_sk#3)]
PushedFilters: [IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_store_sk:int,ss_sales_price:decimal(7,2)>

(23) ColumnarToRow [codegen id : 6]
Input [3]: [ss_store_sk#1, ss_sales_price#2, ss_sold_date_sk#3]

(24) Filter [codegen id : 6]
Input [3]: [ss_store_sk#1, ss_sales_price#2, ss_sold_date_sk#3]
Condition : isnotnull(ss_store_sk#1)

(25) Scan parquet spark_catalog.default.date_dim
Output [3]: [d_date_sk#4, d_week_seq#5, d_day_name#6]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_date_sk), IsNotNull(d_week_seq)]
ReadSchema: struct<d_date_sk:int,d_week_seq:int,d_day_name:string>

(26) ColumnarToRow [codegen id : 5]
Input [3]: [d_date_sk#4, d_week_seq#5, d_day_name#6]

(27) Filter [codegen id : 5]
Input [3]: [d_date_sk#4, d_week_seq#5, d_day_name#6]
Condition : ((isnotnull(d_date_sk#4) AND isnotnull(d_week_seq#5)) AND dynamicpruningexpression(d_week_seq#5 IN dynamicpruning#50))

(28) BroadcastExchange
Input [3]: [d_date_sk#4, d_week_seq#5, d_day_name#6]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=4]

(29) BroadcastHashJoin [codegen id : 6]
Left keys [1]: [ss_sold_date_sk#3]
Right keys [1]: [d_date_sk#4]
Join type: Inner
Join condition: None

(30) Project [codegen id : 6]
Output [4]: [ss_store_sk#1, ss_sales_price#2, d_week_seq#5, d_day_name#6]
Input [6]: [ss_store_sk#1, ss_sales_price#2, ss_sold_date_sk#3, d_date_sk#4, d_week_seq#5, d_day_name#6]

(31) HashAggregate [codegen id : 6]
Input [4]: [ss_store_sk#1, ss_sales_price#2, d_week_seq#5, d_day_name#6]
Keys [2]: [d_week_seq#5, ss_store_sk#1]
Functions [6]: [partial_sum(UnscaledValue(CASE WHEN (d_day_name#6 = Sunday   ) THEN ss_sales_price#2 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#6 = Monday   ) THEN ss_sales_price#2 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#6 = Wednesday) THEN ss_sales_price#2 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#6 = Thursday ) THEN ss_sales_price#2 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#6 = Friday   ) THEN ss_sales_price#2 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#6 = Saturday ) THEN ss_sales_price#2 END))]
Aggregate Attributes [6]: [sum#51, sum#52, sum#53, sum#54, sum#55, sum#56]
Results [8]: [d_week_seq#5, ss_store_sk#1, sum#57, sum#58, sum#59, sum#60, sum#61, sum#62]

(32) Exchange
Input [8]: [d_week_seq#5, ss_store_sk#1, sum#57, sum#58, sum#59, sum#60, sum#61, sum#62]
Arguments: hashpartitioning(d_week_seq#5, ss_store_sk#1, 5), ENSURE_REQUIREMENTS, [plan_id=5]

(33) HashAggregate [codegen id : 9]
Input [8]: [d_week_seq#5, ss_store_sk#1, sum#57, sum#58, sum#59, sum#60, sum#61, sum#62]
Keys [2]: [d_week_seq#5, ss_store_sk#1]
Functions [6]: [sum(UnscaledValue(CASE WHEN (d_day_name#6 = Sunday   ) THEN ss_sales_price#2 END)), sum(UnscaledValue(CASE WHEN (d_day_name#6 = Monday   ) THEN ss_sales_price#2 END)), sum(UnscaledValue(CASE WHEN (d_day_name#6 = Wednesday) THEN ss_sales_price#2 END)), sum(UnscaledValue(CASE WHEN (d_day_name#6 = Thursday ) THEN ss_sales_price#2 END)), sum(UnscaledValue(CASE WHEN (d_day_name#6 = Friday   ) THEN ss_sales_price#2 END)), sum(UnscaledValue(CASE WHEN (d_day_name#6 = Saturday ) THEN ss_sales_price#2 END))]
Aggregate Attributes [6]: [sum(UnscaledValue(CASE WHEN (d_day_name#6 = Sunday   ) THEN ss_sales_price#2 END))#22, sum(UnscaledValue(CASE WHEN (d_day_name#6 = Monday   ) THEN ss_sales_price#2 END))#23, sum(UnscaledValue(CASE WHEN (d_day_name#6 = Wednesday) THEN ss_sales_price#2 END))#25, sum(UnscaledValue(CASE WHEN (d_day_name#6 = Thursday ) THEN ss_sales_price#2 END))#26, sum(UnscaledValue(CASE WHEN (d_day_name#6 = Friday   ) THEN ss_sales_price#2 END))#27, sum(UnscaledValue(CASE WHEN (d_day_name#6 = Saturday ) THEN ss_sales_price#2 END))#28]
Results [8]: [d_week_seq#5, ss_store_sk#1, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#6 = Sunday   ) THEN ss_sales_price#2 END))#22,17,2) AS sun_sales#29, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#6 = Monday   ) THEN ss_sales_price#2 END))#23,17,2) AS mon_sales#30, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#6 = Wednesday) THEN ss_sales_price#2 END))#25,17,2) AS wed_sales#32, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#6 = Thursday ) THEN ss_sales_price#2 END))#26,17,2) AS thu_sales#33, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#6 = Friday   ) THEN ss_sales_price#2 END))#27,17,2) AS fri_sales#34, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#6 = Saturday ) THEN ss_sales_price#2 END))#28,17,2) AS sat_sales#35]

(34) Scan parquet spark_catalog.default.store
Output [2]: [s_store_sk#63, s_store_id#64]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store]
PushedFilters: [IsNotNull(s_store_sk), IsNotNull(s_store_id)]
ReadSchema: struct<s_store_sk:int,s_store_id:string>

(35) ColumnarToRow [codegen id : 7]
Input [2]: [s_store_sk#63, s_store_id#64]

(36) Filter [codegen id : 7]
Input [2]: [s_store_sk#63, s_store_id#64]
Condition : (isnotnull(s_store_sk#63) AND isnotnull(s_store_id#64))

(37) BroadcastExchange
Input [2]: [s_store_sk#63, s_store_id#64]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=6]

(38) BroadcastHashJoin [codegen id : 9]
Left keys [1]: [ss_store_sk#1]
Right keys [1]: [s_store_sk#63]
Join type: Inner
Join condition: None

(39) Project [codegen id : 9]
Output [8]: [d_week_seq#5, sun_sales#29, mon_sales#30, wed_sales#32, thu_sales#33, fri_sales#34, sat_sales#35, s_store_id#64]
Input [10]: [d_week_seq#5, ss_store_sk#1, sun_sales#29, mon_sales#30, wed_sales#32, thu_sales#33, fri_sales#34, sat_sales#35, s_store_sk#63, s_store_id#64]

(40) ReusedExchange [Reuses operator id: 56]
Output [1]: [d_week_seq#65]

(41) BroadcastHashJoin [codegen id : 9]
Left keys [1]: [d_week_seq#5]
Right keys [1]: [d_week_seq#65]
Join type: Inner
Join condition: None

(42) Project [codegen id : 9]
Output [8]: [d_week_seq#5 AS d_week_seq2#66, s_store_id#64 AS s_store_id2#67, sun_sales#29 AS sun_sales2#68, mon_sales#30 AS mon_sales2#69, wed_sales#32 AS wed_sales2#70, thu_sales#33 AS thu_sales2#71, fri_sales#34 AS fri_sales2#72, sat_sales#35 AS sat_sales2#73]
Input [9]: [d_week_seq#5, sun_sales#29, mon_sales#30, wed_sales#32, thu_sales#33, fri_sales#34, sat_sales#35, s_store_id#64, d_week_seq#65]

(43) BroadcastExchange
Input [8]: [d_week_seq2#66, s_store_id2#67, sun_sales2#68, mon_sales2#69, wed_sales2#70, thu_sales2#71, fri_sales2#72, sat_sales2#73]
Arguments: HashedRelationBroadcastMode(List(input[1, string, true], (input[0, int, true] - 52)),false), [plan_id=7]

(44) BroadcastHashJoin [codegen id : 10]
Left keys [2]: [s_store_id1#42, d_week_seq1#41]
Right keys [2]: [s_store_id2#67, (d_week_seq2#66 - 52)]
Join type: Inner
Join condition: None

(45) Project [codegen id : 10]
Output [10]: [s_store_name1#40, s_store_id1#42, d_week_seq1#41, (sun_sales1#43 / sun_sales2#68) AS (sun_sales1 / sun_sales2)#74, (mon_sales1#44 / mon_sales2#69) AS (mon_sales1 / mon_sales2)#75, (tue_sales1#45 / tue_sales1#45) AS (tue_sales1 / tue_sales1)#76, (wed_sales1#46 / wed_sales2#70) AS (wed_sales1 / wed_sales2)#77, (thu_sales1#47 / thu_sales2#71) AS (thu_sales1 / thu_sales2)#78, (fri_sales1#48 / fri_sales2#72) AS (fri_sales1 / fri_sales2)#79, (sat_sales1#49 / sat_sales2#73) AS (sat_sales1 / sat_sales2)#80]
Input [18]: [s_store_name1#40, d_week_seq1#41, s_store_id1#42, sun_sales1#43, mon_sales1#44, tue_sales1#45, wed_sales1#46, thu_sales1#47, fri_sales1#48, sat_sales1#49, d_week_seq2#66, s_store_id2#67, sun_sales2#68, mon_sales2#69, wed_sales2#70, thu_sales2#71, fri_sales2#72, sat_sales2#73]

(46) TakeOrderedAndProject
Input [10]: [s_store_name1#40, s_store_id1#42, d_week_seq1#41, (sun_sales1 / sun_sales2)#74, (mon_sales1 / mon_sales2)#75, (tue_sales1 / tue_sales1)#76, (wed_sales1 / wed_sales2)#77, (thu_sales1 / thu_sales2)#78, (fri_sales1 / fri_sales2)#79, (sat_sales1 / sat_sales2)#80]
Arguments: 100, [s_store_name1#40 ASC NULLS FIRST, s_store_id1#42 ASC NULLS FIRST, d_week_seq1#41 ASC NULLS FIRST], [s_store_name1#40, s_store_id1#42, d_week_seq1#41, (sun_sales1 / sun_sales2)#74, (mon_sales1 / mon_sales2)#75, (tue_sales1 / tue_sales1)#76, (wed_sales1 / wed_sales2)#77, (thu_sales1 / thu_sales2)#78, (fri_sales1 / fri_sales2)#79, (sat_sales1 / sat_sales2)#80]

===== Subqueries =====

Subquery:1 Hosting operator id = 6 Hosting Expression = d_week_seq#5 IN dynamicpruning#7
BroadcastExchange (51)
+- * Project (50)
   +- * Filter (49)
      +- * ColumnarToRow (48)
         +- Scan parquet spark_catalog.default.date_dim (47)


(47) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_month_seq#81, d_week_seq#39]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1185), LessThanOrEqual(d_month_seq,1196), IsNotNull(d_week_seq)]
ReadSchema: struct<d_month_seq:int,d_week_seq:int>

(48) ColumnarToRow [codegen id : 1]
Input [2]: [d_month_seq#81, d_week_seq#39]

(49) Filter [codegen id : 1]
Input [2]: [d_month_seq#81, d_week_seq#39]
Condition : (((isnotnull(d_month_seq#81) AND (d_month_seq#81 >= 1185)) AND (d_month_seq#81 <= 1196)) AND isnotnull(d_week_seq#39))

(50) Project [codegen id : 1]
Output [1]: [d_week_seq#39]
Input [2]: [d_month_seq#81, d_week_seq#39]

(51) BroadcastExchange
Input [1]: [d_week_seq#39]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=8]

Subquery:2 Hosting operator id = 27 Hosting Expression = d_week_seq#5 IN dynamicpruning#50
BroadcastExchange (56)
+- * Project (55)
   +- * Filter (54)
      +- * ColumnarToRow (53)
         +- Scan parquet spark_catalog.default.date_dim (52)


(52) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_month_seq#82, d_week_seq#65]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1197), LessThanOrEqual(d_month_seq,1208), IsNotNull(d_week_seq)]
ReadSchema: struct<d_month_seq:int,d_week_seq:int>

(53) ColumnarToRow [codegen id : 1]
Input [2]: [d_month_seq#82, d_week_seq#65]

(54) Filter [codegen id : 1]
Input [2]: [d_month_seq#82, d_week_seq#65]
Condition : (((isnotnull(d_month_seq#82) AND (d_month_seq#82 >= 1197)) AND (d_month_seq#82 <= 1208)) AND isnotnull(d_week_seq#65))

(55) Project [codegen id : 1]
Output [1]: [d_week_seq#65]
Input [2]: [d_month_seq#82, d_week_seq#65]

(56) BroadcastExchange
Input [1]: [d_week_seq#65]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=9]


